2025-08-21 11:55:43,532 - INFO - Initializing Code Summarizer...
2025-08-21 11:55:43,695 - INFO - Using device: cuda
2025-08-21 11:55:43,696 - INFO - Loading tokenizer: meta-llama/Llama-3.1-8B-Instruct
2025-08-21 11:55:44,970 - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct
2025-08-21 11:56:49,450 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:36, 12.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:23<00:23, 11.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:35<00:11, 11.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:37<00:00,  7.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:37<00:00,  9.36s/it]
2025-08-21 11:57:27,235 - INFO - Applying unstructured pruning with 20.0% weight removal
2025-08-21 11:57:27,235 - INFO - Using CPU offloading pruning to reduce memory pressure
2025-08-21 11:57:27,235 - INFO - Applying pruning with CPU offloading for 20.0% sparsity
2025-08-21 11:57:27,236 - INFO - Processing 225 linear layers with offloading
2025-08-21 11:57:27,236 - INFO - Processing group 1/12 (20 layers)
2025-08-21 11:57:51,690 - INFO - Processing group 2/12 (20 layers)
2025-08-21 11:58:14,863 - INFO - Processing group 3/12 (20 layers)
2025-08-21 11:58:38,287 - INFO - Processing group 4/12 (20 layers)
2025-08-21 11:59:03,547 - INFO - Processing group 5/12 (20 layers)
2025-08-21 11:59:29,615 - INFO - Processing group 6/12 (20 layers)
2025-08-21 11:59:34,753 - WARNING - Failed to prune layer model.layers.14.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 40.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 20.98 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 55.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:36,774 - WARNING - Failed to prune layer model.layers.14.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 40.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 20.98 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 55.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:39,304 - WARNING - Failed to prune layer model.layers.14.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 40.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 20.98 GiB memory in use. Of the allocated memory 20.72 GiB is allocated by PyTorch, and 55.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:40,314 - WARNING - Failed to prune layer model.layers.15.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:40,882 - WARNING - Failed to prune layer model.layers.15.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:42,779 - WARNING - Failed to prune layer model.layers.15.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:44,839 - WARNING - Failed to prune layer model.layers.15.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:47,121 - WARNING - Failed to prune layer model.layers.15.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:47,712 - WARNING - Failed to prune layer model.layers.16.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:47,871 - WARNING - Failed to prune layer model.layers.16.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:48,033 - WARNING - Failed to prune layer model.layers.16.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:48,687 - WARNING - Failed to prune layer model.layers.16.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:50,642 - WARNING - Failed to prune layer model.layers.16.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:53,129 - WARNING - Failed to prune layer model.layers.16.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:55,568 - WARNING - Failed to prune layer model.layers.16.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:56,162 - WARNING - Failed to prune layer model.layers.17.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:56,163 - INFO - Processing group 7/12 (20 layers)
2025-08-21 11:59:56,325 - WARNING - Failed to prune layer model.layers.17.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:56,472 - WARNING - Failed to prune layer model.layers.17.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:57,188 - WARNING - Failed to prune layer model.layers.17.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 11:59:59,658 - WARNING - Failed to prune layer model.layers.17.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:02,056 - WARNING - Failed to prune layer model.layers.17.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:04,319 - WARNING - Failed to prune layer model.layers.17.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:04,957 - WARNING - Failed to prune layer model.layers.18.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:05,115 - WARNING - Failed to prune layer model.layers.18.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:05,252 - WARNING - Failed to prune layer model.layers.18.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:05,931 - WARNING - Failed to prune layer model.layers.18.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:08,158 - WARNING - Failed to prune layer model.layers.18.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:10,481 - WARNING - Failed to prune layer model.layers.18.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:13,011 - WARNING - Failed to prune layer model.layers.18.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:13,599 - WARNING - Failed to prune layer model.layers.19.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:13,765 - WARNING - Failed to prune layer model.layers.19.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:13,931 - WARNING - Failed to prune layer model.layers.19.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:14,567 - WARNING - Failed to prune layer model.layers.19.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:17,009 - WARNING - Failed to prune layer model.layers.19.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:19,640 - WARNING - Failed to prune layer model.layers.19.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:21,981 - WARNING - Failed to prune layer model.layers.19.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:21,981 - INFO - Processing group 8/12 (20 layers)
2025-08-21 12:00:22,605 - WARNING - Failed to prune layer model.layers.20.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:22,751 - WARNING - Failed to prune layer model.layers.20.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:22,904 - WARNING - Failed to prune layer model.layers.20.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:23,575 - WARNING - Failed to prune layer model.layers.20.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:26,181 - WARNING - Failed to prune layer model.layers.20.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:28,397 - WARNING - Failed to prune layer model.layers.20.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:30,676 - WARNING - Failed to prune layer model.layers.20.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:31,303 - WARNING - Failed to prune layer model.layers.21.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:31,473 - WARNING - Failed to prune layer model.layers.21.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:31,632 - WARNING - Failed to prune layer model.layers.21.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:32,195 - WARNING - Failed to prune layer model.layers.21.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:34,095 - WARNING - Failed to prune layer model.layers.21.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:36,338 - WARNING - Failed to prune layer model.layers.21.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:38,771 - WARNING - Failed to prune layer model.layers.21.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:39,445 - WARNING - Failed to prune layer model.layers.22.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:39,593 - WARNING - Failed to prune layer model.layers.22.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:39,754 - WARNING - Failed to prune layer model.layers.22.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:40,408 - WARNING - Failed to prune layer model.layers.22.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:42,415 - WARNING - Failed to prune layer model.layers.22.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:44,485 - WARNING - Failed to prune layer model.layers.22.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:44,485 - INFO - Processing group 9/12 (20 layers)
2025-08-21 12:00:46,425 - WARNING - Failed to prune layer model.layers.22.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:47,015 - WARNING - Failed to prune layer model.layers.23.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:47,161 - WARNING - Failed to prune layer model.layers.23.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:47,335 - WARNING - Failed to prune layer model.layers.23.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:48,059 - WARNING - Failed to prune layer model.layers.23.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:50,254 - WARNING - Failed to prune layer model.layers.23.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:52,414 - WARNING - Failed to prune layer model.layers.23.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:54,852 - WARNING - Failed to prune layer model.layers.23.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:55,444 - WARNING - Failed to prune layer model.layers.24.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:55,589 - WARNING - Failed to prune layer model.layers.24.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:55,732 - WARNING - Failed to prune layer model.layers.24.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:56,398 - WARNING - Failed to prune layer model.layers.24.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:00:58,681 - WARNING - Failed to prune layer model.layers.24.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:01,257 - WARNING - Failed to prune layer model.layers.24.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:03,512 - WARNING - Failed to prune layer model.layers.24.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:04,225 - WARNING - Failed to prune layer model.layers.25.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:04,390 - WARNING - Failed to prune layer model.layers.25.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:04,545 - WARNING - Failed to prune layer model.layers.25.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:05,142 - WARNING - Failed to prune layer model.layers.25.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:07,566 - WARNING - Failed to prune layer model.layers.25.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:07,566 - INFO - Processing group 10/12 (20 layers)
2025-08-21 12:01:10,116 - WARNING - Failed to prune layer model.layers.25.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:12,536 - WARNING - Failed to prune layer model.layers.25.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:13,137 - WARNING - Failed to prune layer model.layers.26.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:13,305 - WARNING - Failed to prune layer model.layers.26.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:13,471 - WARNING - Failed to prune layer model.layers.26.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:14,136 - WARNING - Failed to prune layer model.layers.26.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:16,509 - WARNING - Failed to prune layer model.layers.26.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:19,100 - WARNING - Failed to prune layer model.layers.26.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:21,565 - WARNING - Failed to prune layer model.layers.26.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:22,242 - WARNING - Failed to prune layer model.layers.27.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:22,404 - WARNING - Failed to prune layer model.layers.27.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:22,569 - WARNING - Failed to prune layer model.layers.27.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:23,274 - WARNING - Failed to prune layer model.layers.27.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:25,721 - WARNING - Failed to prune layer model.layers.27.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:28,193 - WARNING - Failed to prune layer model.layers.27.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:30,608 - WARNING - Failed to prune layer model.layers.27.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:31,217 - WARNING - Failed to prune layer model.layers.28.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:31,380 - WARNING - Failed to prune layer model.layers.28.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:31,538 - WARNING - Failed to prune layer model.layers.28.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:32,134 - WARNING - Failed to prune layer model.layers.28.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:32,134 - INFO - Processing group 11/12 (20 layers)
2025-08-21 12:01:34,208 - WARNING - Failed to prune layer model.layers.28.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:36,753 - WARNING - Failed to prune layer model.layers.28.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:38,789 - WARNING - Failed to prune layer model.layers.28.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:39,477 - WARNING - Failed to prune layer model.layers.29.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:39,644 - WARNING - Failed to prune layer model.layers.29.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:39,806 - WARNING - Failed to prune layer model.layers.29.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:40,498 - WARNING - Failed to prune layer model.layers.29.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:42,739 - WARNING - Failed to prune layer model.layers.29.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:44,974 - WARNING - Failed to prune layer model.layers.29.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:47,487 - WARNING - Failed to prune layer model.layers.29.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:48,119 - WARNING - Failed to prune layer model.layers.30.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:48,271 - WARNING - Failed to prune layer model.layers.30.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:48,454 - WARNING - Failed to prune layer model.layers.30.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:49,189 - WARNING - Failed to prune layer model.layers.30.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:51,634 - WARNING - Failed to prune layer model.layers.30.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:54,040 - WARNING - Failed to prune layer model.layers.30.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:56,290 - WARNING - Failed to prune layer model.layers.30.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:56,907 - WARNING - Failed to prune layer model.layers.31.self_attn.q_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:57,070 - WARNING - Failed to prune layer model.layers.31.self_attn.k_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:57,230 - WARNING - Failed to prune layer model.layers.31.self_attn.v_proj with offloading: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:01:57,231 - INFO - Processing group 12/12 (5 layers)
2025-08-21 12:01:57,950 - WARNING - Failed to prune layer model.layers.31.self_attn.o_proj with offloading: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:02:00,158 - WARNING - Failed to prune layer model.layers.31.mlp.gate_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:02:02,537 - WARNING - Failed to prune layer model.layers.31.mlp.up_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:02:04,682 - WARNING - Failed to prune layer model.layers.31.mlp.down_proj with offloading: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:02:26,033 - WARNING - Failed to prune layer lm_head with offloading: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-21 12:02:26,033 - INFO - Offloaded pruning completed: 104/225 layers pruned
Traceback (most recent call last):
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 673, in <module>
    main()
    ~~~~^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 654, in main
    summarizer = CodeSummarizer(
        model_name=args.model_name,
        job_id=args.job_id,
        args=args
    )
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 74, in __init__
    self.apply_offloaded_pruning(pruning_amount)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 380, in apply_offloaded_pruning
    self.calculate_sparsity()
    ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 206, in calculate_sparsity
    weight = module.weight_orig * module.weight_mask
             ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.00 MiB is free. Process 369769 has 58.10 GiB memory in use. Including non-PyTorch memory, this process has 21.01 GiB memory in use. Of the allocated memory 20.76 GiB is allocated by PyTorch, and 47.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: compute-2-3: task 0: Exited with exit code 1
