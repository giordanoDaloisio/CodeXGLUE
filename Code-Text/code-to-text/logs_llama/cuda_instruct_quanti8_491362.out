2025-08-13 10:50:31,597 - INFO - Initializing Code Summarizer...
2025-08-13 10:50:31,709 - INFO - Using device: cuda
2025-08-13 10:50:31,710 - INFO - Apply quantization
2025-08-13 10:50:31,710 - INFO - Using 8-bit quantization
2025-08-13 10:50:31,710 - INFO - Loading tokenizer: meta-llama/Llama-3.1-8B-Instruct
2025-08-13 10:50:32,565 - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct
Traceback (most recent call last):
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 349, in <module>
    main()
    ~~~~^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 330, in main
    summarizer = CodeSummarizer(
        model_name=args.model_name,
        job_id=args.job_id,
        args=args
    )
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 58, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        model_name,
        ^^^^^^^^^^^
    ...<3 lines>...
        trust_remote_code=True
        ^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/modeling_utils.py", line 309, in _wrapper
    return func(*args, **kwargs)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/modeling_utils.py", line 4382, in from_pretrained
    hf_quantizer = AutoHfQuantizer.from_config(
        config.quantization_config,
        pre_quantized=pre_quantized,
    )
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/quantizers/auto.py", line 177, in from_config
    return target_cls(quantization_config, **kwargs)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/quantizers/quantizer_quanto.py", line 52, in __init__
    self.post_init()
    ~~~~~~~~~~~~~~^^
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/quantizers/quantizer_quanto.py", line 59, in post_init
    raise ValueError(
    ...<2 lines>...
    )
ValueError: We don't support quantizing the activations with transformers library.Use quanto library for more complex use cases such as activations quantization, calibration and quantization aware training.
srun: error: compute-0-3: task 0: Exited with exit code 1
