2025-08-19 19:30:37,676 - INFO - Initializing Code Summarizer...
2025-08-19 19:30:37,816 - INFO - Using device: cuda
2025-08-19 19:30:37,816 - INFO - Loading tokenizer: meta-llama/Llama-3.1-8B-Instruct
2025-08-19 19:30:38,644 - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct
2025-08-19 19:30:42,361 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]
2025-08-19 19:30:48,696 - INFO - Applying unstructured pruning with 20.0% weight removal
2025-08-19 19:30:48,696 - INFO - Starting unstructured pruning with 20.0% weight removal
2025-08-19 19:30:48,697 - INFO - Found 225 linear layers with 7,504,658,432 total parameters
2025-08-19 19:30:49,924 - INFO - Pruned 50/225 linear layers
2025-08-19 19:30:50,926 - INFO - Pruned 100/225 linear layers
2025-08-19 19:30:51,892 - WARNING - Failed to prune layer model.layers.17.mlp.gate_proj: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.56 GiB is allocated by PyTorch, and 441.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:51,895 - WARNING - Failed to prune layer model.layers.17.mlp.up_proj: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.56 GiB is allocated by PyTorch, and 441.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:51,897 - WARNING - Failed to prune layer model.layers.17.mlp.down_proj: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.56 GiB is allocated by PyTorch, and 441.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,082 - WARNING - Failed to prune layer model.layers.18.mlp.gate_proj: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.52 GiB is allocated by PyTorch, and 483.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,084 - WARNING - Failed to prune layer model.layers.18.mlp.up_proj: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.52 GiB is allocated by PyTorch, and 483.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,086 - WARNING - Failed to prune layer model.layers.18.mlp.down_proj: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.52 GiB is allocated by PyTorch, and 483.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,219 - WARNING - Failed to prune layer model.layers.19.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.46 GiB is allocated by PyTorch, and 547.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,221 - WARNING - Failed to prune layer model.layers.19.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.46 GiB is allocated by PyTorch, and 547.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,225 - WARNING - Failed to prune layer model.layers.19.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.46 GiB is allocated by PyTorch, and 547.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,259 - WARNING - Failed to prune layer model.layers.20.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.50 GiB is allocated by PyTorch, and 499.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,261 - WARNING - Failed to prune layer model.layers.20.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.50 GiB is allocated by PyTorch, and 499.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,263 - WARNING - Failed to prune layer model.layers.20.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 40.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.08 GiB memory in use. Of the allocated memory 29.50 GiB is allocated by PyTorch, and 499.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,283 - WARNING - Failed to prune layer model.layers.21.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.55 GiB is allocated by PyTorch, and 485.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,285 - WARNING - Failed to prune layer model.layers.21.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.55 GiB is allocated by PyTorch, and 485.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,287 - WARNING - Failed to prune layer model.layers.21.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.55 GiB is allocated by PyTorch, and 485.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,288 - WARNING - Failed to prune layer model.layers.22.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.61 GiB is allocated by PyTorch, and 421.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,294 - WARNING - Failed to prune layer model.layers.22.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.65 GiB is allocated by PyTorch, and 389.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,295 - WARNING - Failed to prune layer model.layers.22.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.58 GiB is allocated by PyTorch, and 453.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,297 - WARNING - Failed to prune layer model.layers.22.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.58 GiB is allocated by PyTorch, and 453.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,299 - WARNING - Failed to prune layer model.layers.22.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.58 GiB is allocated by PyTorch, and 453.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,301 - WARNING - Failed to prune layer model.layers.23.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.65 GiB is allocated by PyTorch, and 389.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,306 - WARNING - Failed to prune layer model.layers.23.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.68 GiB is allocated by PyTorch, and 357.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,308 - WARNING - Failed to prune layer model.layers.23.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.61 GiB is allocated by PyTorch, and 421.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,310 - WARNING - Failed to prune layer model.layers.23.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.61 GiB is allocated by PyTorch, and 421.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,311 - WARNING - Failed to prune layer model.layers.23.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.61 GiB is allocated by PyTorch, and 421.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,313 - WARNING - Failed to prune layer model.layers.24.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.68 GiB is allocated by PyTorch, and 357.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,318 - WARNING - Failed to prune layer model.layers.24.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.71 GiB is allocated by PyTorch, and 325.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,320 - WARNING - Failed to prune layer model.layers.24.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.65 GiB is allocated by PyTorch, and 389.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,322 - WARNING - Failed to prune layer model.layers.24.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.65 GiB is allocated by PyTorch, and 389.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,324 - WARNING - Failed to prune layer model.layers.24.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.65 GiB is allocated by PyTorch, and 389.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,326 - WARNING - Failed to prune layer model.layers.25.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.71 GiB is allocated by PyTorch, and 325.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,331 - WARNING - Failed to prune layer model.layers.25.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.74 GiB is allocated by PyTorch, and 293.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,333 - WARNING - Failed to prune layer model.layers.25.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.68 GiB is allocated by PyTorch, and 357.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,334 - WARNING - Failed to prune layer model.layers.25.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.68 GiB is allocated by PyTorch, and 357.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,336 - WARNING - Failed to prune layer model.layers.25.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.68 GiB is allocated by PyTorch, and 357.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,338 - WARNING - Failed to prune layer model.layers.26.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.74 GiB is allocated by PyTorch, and 293.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,343 - WARNING - Failed to prune layer model.layers.26.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.77 GiB is allocated by PyTorch, and 261.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,345 - WARNING - Failed to prune layer model.layers.26.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.71 GiB is allocated by PyTorch, and 325.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,347 - WARNING - Failed to prune layer model.layers.26.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.71 GiB is allocated by PyTorch, and 325.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,349 - WARNING - Failed to prune layer model.layers.26.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.71 GiB is allocated by PyTorch, and 325.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,351 - WARNING - Failed to prune layer model.layers.27.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.77 GiB is allocated by PyTorch, and 261.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,353 - INFO - Pruned 150/225 linear layers
2025-08-19 19:30:52,356 - WARNING - Failed to prune layer model.layers.27.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 229.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,358 - WARNING - Failed to prune layer model.layers.27.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.74 GiB is allocated by PyTorch, and 293.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,359 - WARNING - Failed to prune layer model.layers.27.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.74 GiB is allocated by PyTorch, and 293.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,361 - WARNING - Failed to prune layer model.layers.27.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.74 GiB is allocated by PyTorch, and 293.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,363 - WARNING - Failed to prune layer model.layers.28.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 229.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,368 - WARNING - Failed to prune layer model.layers.28.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.83 GiB is allocated by PyTorch, and 197.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,370 - WARNING - Failed to prune layer model.layers.28.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.77 GiB is allocated by PyTorch, and 261.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,372 - WARNING - Failed to prune layer model.layers.28.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.77 GiB is allocated by PyTorch, and 261.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,374 - WARNING - Failed to prune layer model.layers.28.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.77 GiB is allocated by PyTorch, and 261.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,376 - WARNING - Failed to prune layer model.layers.29.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.83 GiB is allocated by PyTorch, and 197.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,382 - WARNING - Failed to prune layer model.layers.29.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.83 GiB is allocated by PyTorch, and 197.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,383 - WARNING - Failed to prune layer model.layers.29.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 229.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,385 - WARNING - Failed to prune layer model.layers.29.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 229.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,387 - WARNING - Failed to prune layer model.layers.29.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 229.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,389 - WARNING - Failed to prune layer model.layers.30.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.83 GiB is allocated by PyTorch, and 197.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,393 - WARNING - Failed to prune layer model.layers.30.self_attn.v_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.86 GiB is allocated by PyTorch, and 166.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,394 - WARNING - Failed to prune layer model.layers.30.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,396 - WARNING - Failed to prune layer model.layers.30.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.82 GiB is allocated by PyTorch, and 213.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,398 - WARNING - Failed to prune layer model.layers.30.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.82 GiB is allocated by PyTorch, and 213.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,400 - WARNING - Failed to prune layer model.layers.30.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.82 GiB is allocated by PyTorch, and 213.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,402 - WARNING - Failed to prune layer model.layers.31.self_attn.q_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,404 - WARNING - Failed to prune layer model.layers.31.self_attn.k_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.86 GiB is allocated by PyTorch, and 166.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,406 - WARNING - Failed to prune layer model.layers.31.self_attn.v_proj: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.86 GiB is allocated by PyTorch, and 166.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,408 - WARNING - Failed to prune layer model.layers.31.self_attn.o_proj: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,409 - WARNING - Failed to prune layer model.layers.31.mlp.gate_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.82 GiB is allocated by PyTorch, and 213.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,411 - WARNING - Failed to prune layer model.layers.31.mlp.up_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.82 GiB is allocated by PyTorch, and 213.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,413 - WARNING - Failed to prune layer model.layers.31.mlp.down_proj: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.82 GiB is allocated by PyTorch, and 213.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,415 - WARNING - Failed to prune layer lm_head: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.82 GiB is allocated by PyTorch, and 213.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-08-19 19:30:52,415 - INFO - Successfully pruned 156/225 linear layers
2025-08-19 19:30:52,415 - INFO - Total pruned weights: 797,756,553
Traceback (most recent call last):
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 474, in <module>
    main()
    ~~~~^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 455, in main
    summarizer = CodeSummarizer(
        model_name=args.model_name,
        job_id=args.job_id,
        args=args
    )
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 71, in __init__
    self.apply_unstructured_pruning(pruning_amount)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 133, in apply_unstructured_pruning
    self.calculate_sparsity()
    ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Text/code-to-text/code/code_summarization_llama.py", line 151, in calculate_sparsity
    zero_params += (weight == 0).sum().item()
                    ^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 39.25 GiB of which 6.31 MiB is free. Process 65666 has 9.08 GiB memory in use. Including non-PyTorch memory, this process has 30.11 GiB memory in use. Of the allocated memory 29.85 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
srun: error: compute-0-3: task 0: Exited with exit code 1
