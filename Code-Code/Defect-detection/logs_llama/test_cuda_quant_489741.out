WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Device:  cuda
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 40.30it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:18,  9.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:07,  7.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  4.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.25s/it]
Some weights of the model checkpoint at meta-llama/Llama-3.1-8B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized because the shapes did not match:
- model.embed_tokens.weight: found shape torch.Size([128256, 4096]) in the checkpoint and torch.Size([128257, 4096]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/43 [00:00<?, ?it/s]  2%|▏         | 1/43 [00:15<10:42, 15.31s/it]  5%|▍         | 2/43 [00:23<07:28, 10.95s/it]  7%|▋         | 3/43 [00:31<06:21,  9.54s/it]  9%|▉         | 4/43 [00:38<05:46,  8.88s/it] 12%|█▏        | 5/43 [00:46<05:23,  8.52s/it] 14%|█▍        | 6/43 [00:54<05:04,  8.23s/it] 16%|█▋        | 7/43 [01:02<04:49,  8.04s/it] 19%|█▊        | 8/43 [01:09<04:37,  7.92s/it] 21%|██        | 9/43 [01:17<04:26,  7.84s/it] 23%|██▎       | 10/43 [01:25<04:16,  7.79s/it] 26%|██▌       | 11/43 [01:32<04:07,  7.75s/it] 28%|██▊       | 12/43 [01:40<03:59,  7.72s/it] 30%|███       | 13/43 [01:48<03:51,  7.70s/it] 33%|███▎      | 14/43 [01:55<03:43,  7.69s/it] 35%|███▍      | 15/43 [02:03<03:35,  7.68s/it] 37%|███▋      | 16/43 [02:11<03:27,  7.70s/it] 40%|███▉      | 17/43 [02:19<03:21,  7.75s/it] 42%|████▏     | 18/43 [02:26<03:14,  7.78s/it] 44%|████▍     | 19/43 [02:34<03:07,  7.81s/it] 47%|████▋     | 20/43 [02:42<02:59,  7.82s/it] 49%|████▉     | 21/43 [02:50<02:51,  7.81s/it] 51%|█████     | 22/43 [02:58<02:43,  7.77s/it] 53%|█████▎    | 23/43 [03:05<02:35,  7.77s/it] 56%|█████▌    | 24/43 [03:13<02:28,  7.81s/it] 58%|█████▊    | 25/43 [03:21<02:21,  7.84s/it] 60%|██████    | 26/43 [03:29<02:12,  7.81s/it] 63%|██████▎   | 27/43 [03:37<02:04,  7.77s/it] 65%|██████▌   | 28/43 [03:44<01:56,  7.74s/it] 67%|██████▋   | 29/43 [03:52<01:48,  7.72s/it] 70%|██████▉   | 30/43 [04:00<01:40,  7.70s/it] 72%|███████▏  | 31/43 [04:07<01:32,  7.70s/it] 74%|███████▍  | 32/43 [04:15<01:24,  7.70s/it] 77%|███████▋  | 33/43 [04:23<01:16,  7.70s/it] 79%|███████▉  | 34/43 [04:31<01:09,  7.75s/it] 81%|████████▏ | 35/43 [04:38<01:02,  7.78s/it] 84%|████████▎ | 36/43 [04:46<00:54,  7.81s/it] 86%|████████▌ | 37/43 [04:54<00:46,  7.83s/it] 88%|████████▊ | 38/43 [05:02<00:39,  7.84s/it] 91%|█████████ | 39/43 [05:10<00:31,  7.85s/it] 93%|█████████▎| 40/43 [05:18<00:23,  7.85s/it] 95%|█████████▌| 41/43 [05:26<00:15,  7.86s/it] 98%|█████████▊| 42/43 [05:33<00:07,  7.86s/it]100%|██████████| 43/43 [05:39<00:00,  7.21s/it]100%|██████████| 43/43 [05:39<00:00,  7.90s/it]
Size (MB): 9116.625569
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForSequenceClassification                          --
├─LlamaModel: 1-1                                       --
│    └─embed_tokens.weight                              ├─525,340,672
│    └─layers.0.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.0.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.0.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.0.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.0.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.0.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.0.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.0.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.0.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.0.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.0.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.0.input_layernorm.weight                  ├─4,096
│    └─layers.0.post_attention_layernorm.weight         ├─4,096
│    └─layers.1.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.1.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.1.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.1.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.1.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.1.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.1.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.1.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.1.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.1.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.1.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.1.input_layernorm.weight                  ├─4,096
│    └─layers.1.post_attention_layernorm.weight         ├─4,096
│    └─layers.2.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.2.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.2.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.2.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.2.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.2.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.2.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.2.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.2.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.2.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.2.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.2.input_layernorm.weight                  ├─4,096
│    └─layers.2.post_attention_layernorm.weight         ├─4,096
│    └─layers.3.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.3.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.3.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.3.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.3.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.3.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.3.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.3.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.3.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.3.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.3.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.3.input_layernorm.weight                  ├─4,096
│    └─layers.3.post_attention_layernorm.weight         ├─4,096
│    └─layers.4.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.4.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.4.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.4.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.4.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.4.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.4.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.4.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.4.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.4.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.4.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.4.input_layernorm.weight                  ├─4,096
│    └─layers.4.post_attention_layernorm.weight         ├─4,096
│    └─layers.5.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.5.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.5.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.5.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.5.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.5.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.5.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.5.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.5.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.5.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.5.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.5.input_layernorm.weight                  ├─4,096
│    └─layers.5.post_attention_layernorm.weight         ├─4,096
│    └─layers.6.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.6.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.6.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.6.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.6.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.6.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.6.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.6.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.6.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.6.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.6.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.6.input_layernorm.weight                  ├─4,096
│    └─layers.6.post_attention_layernorm.weight         ├─4,096
│    └─layers.7.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.7.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.7.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.7.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.7.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.7.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.7.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.7.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.7.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.7.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.7.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.7.input_layernorm.weight                  ├─4,096
│    └─layers.7.post_attention_layernorm.weight         ├─4,096
│    └─layers.8.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.8.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.8.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.8.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.8.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.8.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.8.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.8.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.8.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.8.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.8.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.8.input_layernorm.weight                  ├─4,096
│    └─layers.8.post_attention_layernorm.weight         ├─4,096
│    └─layers.9.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.9.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.9.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.9.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.9.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.9.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.9.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.9.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.9.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.9.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.9.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.9.input_layernorm.weight                  ├─4,096
│    └─layers.9.post_attention_layernorm.weight         ├─4,096
│    └─layers.10.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.10.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.10.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.10.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.10.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.10.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.10.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.10.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.10.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.10.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.10.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.10.input_layernorm.weight                 ├─4,096
│    └─layers.10.post_attention_layernorm.weight        ├─4,096
│    └─layers.11.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.11.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.11.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.11.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.11.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.11.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.11.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.11.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.11.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.11.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.11.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.11.input_layernorm.weight                 ├─4,096
│    └─layers.11.post_attention_layernorm.weight        ├─4,096
│    └─layers.12.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.12.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.12.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.12.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.12.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.12.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.12.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.12.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.12.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.12.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.12.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.12.input_layernorm.weight                 ├─4,096
│    └─layers.12.post_attention_layernorm.weight        ├─4,096
│    └─layers.13.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.13.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.13.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.13.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.13.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.13.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.13.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.13.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.13.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.13.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.13.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.13.input_layernorm.weight                 ├─4,096
│    └─layers.13.post_attention_layernorm.weight        ├─4,096
│    └─layers.14.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.14.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.14.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.14.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.14.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.14.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.14.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.14.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.14.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.14.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.14.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.14.input_layernorm.weight                 ├─4,096
│    └─layers.14.post_attention_layernorm.weight        ├─4,096
│    └─layers.15.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.15.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.15.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.15.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.15.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.15.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.15.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.15.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.15.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.15.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.15.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.15.input_layernorm.weight                 ├─4,096
│    └─layers.15.post_attention_layernorm.weight        ├─4,096
│    └─layers.16.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.16.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.16.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.16.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.16.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.16.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.16.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.16.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.16.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.16.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.16.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.16.input_layernorm.weight                 ├─4,096
│    └─layers.16.post_attention_layernorm.weight        ├─4,096
│    └─layers.17.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.17.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.17.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.17.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.17.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.17.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.17.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.17.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.17.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.17.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.17.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.17.input_layernorm.weight                 ├─4,096
│    └─layers.17.post_attention_layernorm.weight        ├─4,096
│    └─layers.18.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.18.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.18.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.18.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.18.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.18.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.18.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.18.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.18.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.18.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.18.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.18.input_layernorm.weight                 ├─4,096
│    └─layers.18.post_attention_layernorm.weight        ├─4,096
│    └─layers.19.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.19.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.19.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.19.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.19.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.19.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.19.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.19.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.19.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.19.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.19.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.19.input_layernorm.weight                 ├─4,096
│    └─layers.19.post_attention_layernorm.weight        ├─4,096
│    └─layers.20.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.20.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.20.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.20.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.20.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.20.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.20.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.20.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.20.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.20.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.20.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.20.input_layernorm.weight                 ├─4,096
│    └─layers.20.post_attention_layernorm.weight        ├─4,096
│    └─layers.21.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.21.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.21.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.21.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.21.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.21.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.21.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.21.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.21.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.21.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.21.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.21.input_layernorm.weight                 ├─4,096
│    └─layers.21.post_attention_layernorm.weight        ├─4,096
│    └─layers.22.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.22.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.22.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.22.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.22.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.22.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.22.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.22.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.22.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.22.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.22.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.22.input_layernorm.weight                 ├─4,096
│    └─layers.22.post_attention_layernorm.weight        ├─4,096
│    └─layers.23.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.23.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.23.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.23.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.23.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.23.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.23.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.23.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.23.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.23.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.23.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.23.input_layernorm.weight                 ├─4,096
│    └─layers.23.post_attention_layernorm.weight        ├─4,096
│    └─layers.24.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.24.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.24.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.24.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.24.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.24.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.24.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.24.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.24.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.24.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.24.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.24.input_layernorm.weight                 ├─4,096
│    └─layers.24.post_attention_layernorm.weight        ├─4,096
│    └─layers.25.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.25.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.25.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.25.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.25.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.25.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.25.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.25.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.25.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.25.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.25.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.25.input_layernorm.weight                 ├─4,096
│    └─layers.25.post_attention_layernorm.weight        ├─4,096
│    └─layers.26.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.26.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.26.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.26.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.26.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.26.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.26.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.26.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.26.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.26.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.26.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.26.input_layernorm.weight                 ├─4,096
│    └─layers.26.post_attention_layernorm.weight        ├─4,096
│    └─layers.27.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.27.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.27.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.27.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.27.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.27.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.27.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.27.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.27.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.27.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.27.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.27.input_layernorm.weight                 ├─4,096
│    └─layers.27.post_attention_layernorm.weight        ├─4,096
│    └─layers.28.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.28.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.28.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.28.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.28.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.28.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.28.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.28.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.28.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.28.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.28.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.28.input_layernorm.weight                 ├─4,096
│    └─layers.28.post_attention_layernorm.weight        ├─4,096
│    └─layers.29.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.29.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.29.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.29.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.29.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.29.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.29.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.29.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.29.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.29.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.29.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.29.input_layernorm.weight                 ├─4,096
│    └─layers.29.post_attention_layernorm.weight        ├─4,096
│    └─layers.30.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.30.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.30.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.30.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.30.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.30.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.30.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.30.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.30.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.30.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.30.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.30.input_layernorm.weight                 ├─4,096
│    └─layers.30.post_attention_layernorm.weight        ├─4,096
│    └─layers.31.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.31.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.31.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.31.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.31.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.31.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.31.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.31.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.31.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.31.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.31.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.31.input_layernorm.weight                 ├─4,096
│    └─layers.31.post_attention_layernorm.weight        ├─4,096
│    └─norm.weight                                      └─4,096
│    └─Embedding: 2-1                                   (525,340,672)
│    │    └─weight                                      └─525,340,672
│    └─ModuleList: 2-2                                  --
│    │    └─0.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─0.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─0.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─0.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─0.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─0.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─0.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─0.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─0.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─0.mlp.up_proj.weight                        ├─58,720,256
│    │    └─0.mlp.down_proj.weight                      ├─58,720,256
│    │    └─0.input_layernorm.weight                    ├─4,096
│    │    └─0.post_attention_layernorm.weight           ├─4,096
│    │    └─1.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─1.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─1.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─1.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─1.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─1.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─1.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─1.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─1.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─1.mlp.up_proj.weight                        ├─58,720,256
│    │    └─1.mlp.down_proj.weight                      ├─58,720,256
│    │    └─1.input_layernorm.weight                    ├─4,096
│    │    └─1.post_attention_layernorm.weight           ├─4,096
│    │    └─2.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─2.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─2.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─2.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─2.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─2.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─2.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─2.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─2.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─2.mlp.up_proj.weight                        ├─58,720,256
│    │    └─2.mlp.down_proj.weight                      ├─58,720,256
│    │    └─2.input_layernorm.weight                    ├─4,096
│    │    └─2.post_attention_layernorm.weight           ├─4,096
│    │    └─3.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─3.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─3.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─3.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─3.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─3.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─3.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─3.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─3.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─3.mlp.up_proj.weight                        ├─58,720,256
│    │    └─3.mlp.down_proj.weight                      ├─58,720,256
│    │    └─3.input_layernorm.weight                    ├─4,096
│    │    └─3.post_attention_layernorm.weight           ├─4,096
│    │    └─4.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─4.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─4.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─4.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─4.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─4.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─4.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─4.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─4.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─4.mlp.up_proj.weight                        ├─58,720,256
│    │    └─4.mlp.down_proj.weight                      ├─58,720,256
│    │    └─4.input_layernorm.weight                    ├─4,096
│    │    └─4.post_attention_layernorm.weight           ├─4,096
│    │    └─5.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─5.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─5.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─5.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─5.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─5.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─5.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─5.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─5.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─5.mlp.up_proj.weight                        ├─58,720,256
│    │    └─5.mlp.down_proj.weight                      ├─58,720,256
│    │    └─5.input_layernorm.weight                    ├─4,096
│    │    └─5.post_attention_layernorm.weight           ├─4,096
│    │    └─6.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─6.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─6.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─6.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─6.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─6.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─6.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─6.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─6.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─6.mlp.up_proj.weight                        ├─58,720,256
│    │    └─6.mlp.down_proj.weight                      ├─58,720,256
│    │    └─6.input_layernorm.weight                    ├─4,096
│    │    └─6.post_attention_layernorm.weight           ├─4,096
│    │    └─7.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─7.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─7.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─7.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─7.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─7.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─7.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─7.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─7.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─7.mlp.up_proj.weight                        ├─58,720,256
│    │    └─7.mlp.down_proj.weight                      ├─58,720,256
│    │    └─7.input_layernorm.weight                    ├─4,096
│    │    └─7.post_attention_layernorm.weight           ├─4,096
│    │    └─8.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─8.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─8.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─8.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─8.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─8.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─8.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─8.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─8.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─8.mlp.up_proj.weight                        ├─58,720,256
│    │    └─8.mlp.down_proj.weight                      ├─58,720,256
│    │    └─8.input_layernorm.weight                    ├─4,096
│    │    └─8.post_attention_layernorm.weight           ├─4,096
│    │    └─9.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─9.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─9.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─9.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─9.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─9.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─9.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─9.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─9.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─9.mlp.up_proj.weight                        ├─58,720,256
│    │    └─9.mlp.down_proj.weight                      ├─58,720,256
│    │    └─9.input_layernorm.weight                    ├─4,096
│    │    └─9.post_attention_layernorm.weight           ├─4,096
│    │    └─10.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─10.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─10.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─10.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─10.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─10.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─10.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─10.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─10.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─10.mlp.up_proj.weight                       ├─58,720,256
│    │    └─10.mlp.down_proj.weight                     ├─58,720,256
│    │    └─10.input_layernorm.weight                   ├─4,096
│    │    └─10.post_attention_layernorm.weight          ├─4,096
│    │    └─11.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─11.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─11.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─11.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─11.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─11.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─11.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─11.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─11.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─11.mlp.up_proj.weight                       ├─58,720,256
│    │    └─11.mlp.down_proj.weight                     ├─58,720,256
│    │    └─11.input_layernorm.weight                   ├─4,096
│    │    └─11.post_attention_layernorm.weight          ├─4,096
│    │    └─12.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─12.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─12.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─12.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─12.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─12.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─12.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─12.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─12.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─12.mlp.up_proj.weight                       ├─58,720,256
│    │    └─12.mlp.down_proj.weight                     ├─58,720,256
│    │    └─12.input_layernorm.weight                   ├─4,096
│    │    └─12.post_attention_layernorm.weight          ├─4,096
│    │    └─13.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─13.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─13.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─13.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─13.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─13.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─13.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─13.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─13.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─13.mlp.up_proj.weight                       ├─58,720,256
│    │    └─13.mlp.down_proj.weight                     ├─58,720,256
│    │    └─13.input_layernorm.weight                   ├─4,096
│    │    └─13.post_attention_layernorm.weight          ├─4,096
│    │    └─14.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─14.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─14.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─14.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─14.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─14.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─14.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─14.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─14.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─14.mlp.up_proj.weight                       ├─58,720,256
│    │    └─14.mlp.down_proj.weight                     ├─58,720,256
│    │    └─14.input_layernorm.weight                   ├─4,096
│    │    └─14.post_attention_layernorm.weight          ├─4,096
│    │    └─15.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─15.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─15.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─15.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─15.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─15.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─15.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─15.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─15.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─15.mlp.up_proj.weight                       ├─58,720,256
│    │    └─15.mlp.down_proj.weight                     ├─58,720,256
│    │    └─15.input_layernorm.weight                   ├─4,096
│    │    └─15.post_attention_layernorm.weight          ├─4,096
│    │    └─16.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─16.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─16.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─16.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─16.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─16.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─16.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─16.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─16.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─16.mlp.up_proj.weight                       ├─58,720,256
│    │    └─16.mlp.down_proj.weight                     ├─58,720,256
│    │    └─16.input_layernorm.weight                   ├─4,096
│    │    └─16.post_attention_layernorm.weight          ├─4,096
│    │    └─17.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─17.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─17.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─17.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─17.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─17.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─17.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─17.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─17.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─17.mlp.up_proj.weight                       ├─58,720,256
│    │    └─17.mlp.down_proj.weight                     ├─58,720,256
│    │    └─17.input_layernorm.weight                   ├─4,096
│    │    └─17.post_attention_layernorm.weight          ├─4,096
│    │    └─18.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─18.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─18.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─18.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─18.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─18.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─18.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─18.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─18.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─18.mlp.up_proj.weight                       ├─58,720,256
│    │    └─18.mlp.down_proj.weight                     ├─58,720,256
│    │    └─18.input_layernorm.weight                   ├─4,096
│    │    └─18.post_attention_layernorm.weight          ├─4,096
│    │    └─19.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─19.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─19.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─19.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─19.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─19.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─19.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─19.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─19.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─19.mlp.up_proj.weight                       ├─58,720,256
│    │    └─19.mlp.down_proj.weight                     ├─58,720,256
│    │    └─19.input_layernorm.weight                   ├─4,096
│    │    └─19.post_attention_layernorm.weight          ├─4,096
│    │    └─20.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─20.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─20.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─20.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─20.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─20.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─20.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─20.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─20.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─20.mlp.up_proj.weight                       ├─58,720,256
│    │    └─20.mlp.down_proj.weight                     ├─58,720,256
│    │    └─20.input_layernorm.weight                   ├─4,096
│    │    └─20.post_attention_layernorm.weight          ├─4,096
│    │    └─21.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─21.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─21.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─21.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─21.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─21.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─21.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─21.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─21.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─21.mlp.up_proj.weight                       ├─58,720,256
│    │    └─21.mlp.down_proj.weight                     ├─58,720,256
│    │    └─21.input_layernorm.weight                   ├─4,096
│    │    └─21.post_attention_layernorm.weight          ├─4,096
│    │    └─22.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─22.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─22.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─22.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─22.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─22.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─22.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─22.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─22.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─22.mlp.up_proj.weight                       ├─58,720,256
│    │    └─22.mlp.down_proj.weight                     ├─58,720,256
│    │    └─22.input_layernorm.weight                   ├─4,096
│    │    └─22.post_attention_layernorm.weight          ├─4,096
│    │    └─23.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─23.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─23.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─23.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─23.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─23.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─23.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─23.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─23.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─23.mlp.up_proj.weight                       ├─58,720,256
│    │    └─23.mlp.down_proj.weight                     ├─58,720,256
│    │    └─23.input_layernorm.weight                   ├─4,096
│    │    └─23.post_attention_layernorm.weight          ├─4,096
│    │    └─24.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─24.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─24.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─24.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─24.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─24.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─24.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─24.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─24.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─24.mlp.up_proj.weight                       ├─58,720,256
│    │    └─24.mlp.down_proj.weight                     ├─58,720,256
│    │    └─24.input_layernorm.weight                   ├─4,096
│    │    └─24.post_attention_layernorm.weight          ├─4,096
│    │    └─25.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─25.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─25.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─25.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─25.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─25.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─25.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─25.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─25.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─25.mlp.up_proj.weight                       ├─58,720,256
│    │    └─25.mlp.down_proj.weight                     ├─58,720,256
│    │    └─25.input_layernorm.weight                   ├─4,096
│    │    └─25.post_attention_layernorm.weight          ├─4,096
│    │    └─26.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─26.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─26.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─26.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─26.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─26.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─26.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─26.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─26.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─26.mlp.up_proj.weight                       ├─58,720,256
│    │    └─26.mlp.down_proj.weight                     ├─58,720,256
│    │    └─26.input_layernorm.weight                   ├─4,096
│    │    └─26.post_attention_layernorm.weight          ├─4,096
│    │    └─27.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─27.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─27.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─27.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─27.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─27.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─27.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─27.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─27.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─27.mlp.up_proj.weight                       ├─58,720,256
│    │    └─27.mlp.down_proj.weight                     ├─58,720,256
│    │    └─27.input_layernorm.weight                   ├─4,096
│    │    └─27.post_attention_layernorm.weight          ├─4,096
│    │    └─28.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─28.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─28.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─28.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─28.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─28.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─28.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─28.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─28.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─28.mlp.up_proj.weight                       ├─58,720,256
│    │    └─28.mlp.down_proj.weight                     ├─58,720,256
│    │    └─28.input_layernorm.weight                   ├─4,096
│    │    └─28.post_attention_layernorm.weight          ├─4,096
│    │    └─29.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─29.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─29.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─29.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─29.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─29.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─29.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─29.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─29.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─29.mlp.up_proj.weight                       ├─58,720,256
│    │    └─29.mlp.down_proj.weight                     ├─58,720,256
│    │    └─29.input_layernorm.weight                   ├─4,096
│    │    └─29.post_attention_layernorm.weight          ├─4,096
│    │    └─30.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─30.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─30.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─30.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─30.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─30.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─30.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─30.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─30.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─30.mlp.up_proj.weight                       ├─58,720,256
│    │    └─30.mlp.down_proj.weight                     ├─58,720,256
│    │    └─30.input_layernorm.weight                   ├─4,096
│    │    └─30.post_attention_layernorm.weight          ├─4,096
│    │    └─31.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─31.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─31.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─31.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─31.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─31.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─31.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─31.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─31.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─31.mlp.up_proj.weight                       ├─58,720,256
│    │    └─31.mlp.down_proj.weight                     ├─58,720,256
│    │    └─31.input_layernorm.weight                   ├─4,096
│    │    └─31.post_attention_layernorm.weight          └─4,096
│    │    └─LlamaDecoderLayer: 3-1                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-2                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-3                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-4                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-5                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-6                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-7                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-8                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-9                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-10                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-11                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-12                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-13                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-14                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-15                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-16                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-17                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-18                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-19                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-20                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-21                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-22                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-23                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-24                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-25                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-26                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-27                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-28                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-29                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-30                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-31                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-32                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    └─LlamaRMSNorm: 2-3                                (4,096)
│    │    └─weight                                      └─4,096
│    └─LlamaRotaryEmbedding: 2-4                        --
├─QLinear: 1-2                                          8,192
│    └─weight                                           └─8,192
================================================================================
Total params: 7,532,199,936
Trainable params: 7,006,593,024
Non-trainable params: 525,606,912
================================================================================
  0%|          | 0/43 [00:00<?, ?it/s]  2%|▏         | 1/43 [00:07<05:12,  7.43s/it]  5%|▍         | 2/43 [00:13<04:38,  6.79s/it]  7%|▋         | 3/43 [00:20<04:22,  6.57s/it]  9%|▉         | 4/43 [00:26<04:12,  6.47s/it] 12%|█▏        | 5/43 [00:32<04:03,  6.41s/it] 14%|█▍        | 6/43 [00:39<03:56,  6.38s/it] 16%|█▋        | 7/43 [00:45<03:48,  6.36s/it] 19%|█▊        | 8/43 [00:51<03:42,  6.34s/it] 21%|██        | 9/43 [00:57<03:35,  6.33s/it] 23%|██▎       | 10/43 [01:04<03:28,  6.33s/it] 26%|██▌       | 11/43 [01:10<03:22,  6.32s/it] 28%|██▊       | 12/43 [01:16<03:15,  6.31s/it] 30%|███       | 13/43 [01:23<03:09,  6.31s/it] 33%|███▎      | 14/43 [01:29<03:02,  6.31s/it] 35%|███▍      | 15/43 [01:35<02:56,  6.30s/it] 37%|███▋      | 16/43 [01:42<02:50,  6.31s/it] 40%|███▉      | 17/43 [01:48<02:44,  6.31s/it] 42%|████▏     | 18/43 [01:54<02:37,  6.31s/it] 44%|████▍     | 19/43 [02:01<02:31,  6.31s/it] 47%|████▋     | 20/43 [02:07<02:25,  6.31s/it] 49%|████▉     | 21/43 [02:13<02:18,  6.31s/it] 51%|█████     | 22/43 [02:19<02:12,  6.31s/it] 53%|█████▎    | 23/43 [02:26<02:06,  6.31s/it] 56%|█████▌    | 24/43 [02:32<01:59,  6.31s/it] 58%|█████▊    | 25/43 [02:38<01:53,  6.31s/it] 60%|██████    | 26/43 [02:45<01:47,  6.30s/it] 63%|██████▎   | 27/43 [02:51<01:40,  6.31s/it] 65%|██████▌   | 28/43 [02:57<01:34,  6.31s/it] 67%|██████▋   | 29/43 [03:04<01:28,  6.31s/it] 70%|██████▉   | 30/43 [03:10<01:22,  6.31s/it] 72%|███████▏  | 31/43 [03:16<01:15,  6.31s/it] 74%|███████▍  | 32/43 [03:23<01:09,  6.32s/it] 77%|███████▋  | 33/43 [03:29<01:03,  6.32s/it] 79%|███████▉  | 34/43 [03:35<00:56,  6.32s/it] 81%|████████▏ | 35/43 [03:42<00:50,  6.32s/it] 84%|████████▎ | 36/43 [03:48<00:44,  6.31s/it] 86%|████████▌ | 37/43 [03:54<00:37,  6.31s/it] 88%|████████▊ | 38/43 [04:00<00:31,  6.31s/it] 91%|█████████ | 39/43 [04:07<00:25,  6.31s/it] 93%|█████████▎| 40/43 [04:13<00:18,  6.31s/it] 95%|█████████▌| 41/43 [04:19<00:12,  6.30s/it] 98%|█████████▊| 42/43 [04:26<00:06,  6.31s/it]100%|██████████| 43/43 [04:30<00:00,  5.72s/it]100%|██████████| 43/43 [04:30<00:00,  6.29s/it]
Average inference time: 5.099293231964111
