Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:23<00:23, 11.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:35<00:11, 11.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  7.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:35<00:00,  8.84s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Code/Defect-detection/code/llama_distillation.py", line 32, in <module>
    teacher_model = AutoModelForSequenceClassification.from_pretrained(
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
      "saved_models_llama/final_model",
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
      ignore_mismatched_sizes=True,
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ).to(device)
    ^
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/modeling_utils.py", line 309, in _wrapper
    return func(*args, **kwargs)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/modeling_utils.py", line 4675, in from_pretrained
    model.load_adapter(
    ~~~~~~~~~~~~~~~~~~^
        _adapter_model_path,
        ^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        adapter_kwargs=adapter_kwargs,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/integrations/peft.py", line 239, in load_adapter
    incompatible_keys = set_peft_model_state_dict(
        self, processed_adapter_state_dict, adapter_name, **peft_load_kwargs
    )
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/peft/utils/save_and_load.py", line 448, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/torch/nn/modules/module.py", line 2593, in load_state_dict
    raise RuntimeError(
    ...<3 lines>...
    )
RuntimeError: Error(s) in loading state_dict for LlamaForSequenceClassification:
	size mismatch for model.embed_tokens.weight: copying a param with shape torch.Size([128257, 4096]) from checkpoint, the shape in current model is torch.Size([128256, 4096]).
srun: error: compute-2-3: task 0: Exited with exit code 1
