WARNING:__main__:Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Device:  cuda
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:  25%|██▌       | 1/4 [00:00<00:00,  8.29it/s]Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 32.99it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:19, 10.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  6.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.65s/it]
Some weights of the model checkpoint at meta-llama/Llama-3.1-8B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized because the shapes did not match:
- model.embed_tokens.weight: found shape torch.Size([128256, 4096]) in the checkpoint and torch.Size([128257, 4096]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/43 [00:00<?, ?it/s]  2%|▏         | 1/43 [00:59<41:48, 59.73s/it]  5%|▍         | 2/43 [01:53<38:29, 56.32s/it]  7%|▋         | 3/43 [02:47<36:47, 55.20s/it]  9%|▉         | 4/43 [03:41<35:29, 54.61s/it] 12%|█▏        | 5/43 [04:34<34:23, 54.29s/it] 14%|█▍        | 6/43 [05:28<33:24, 54.16s/it] 16%|█▋        | 7/43 [06:22<32:25, 54.03s/it] 19%|█▊        | 8/43 [07:16<31:27, 53.94s/it] 21%|██        | 9/43 [08:10<30:31, 53.87s/it] 23%|██▎       | 10/43 [09:03<29:36, 53.82s/it] 26%|██▌       | 11/43 [09:57<28:42, 53.81s/it] 28%|██▊       | 12/43 [10:51<27:47, 53.80s/it] 30%|███       | 13/43 [11:45<26:54, 53.82s/it] 33%|███▎      | 14/43 [12:39<26:00, 53.82s/it] 35%|███▍      | 15/43 [13:32<25:06, 53.81s/it] 37%|███▋      | 16/43 [14:26<24:12, 53.81s/it] 40%|███▉      | 17/43 [15:20<23:18, 53.79s/it] 42%|████▏     | 18/43 [16:14<22:24, 53.76s/it] 44%|████▍     | 19/43 [17:07<21:29, 53.74s/it] 47%|████▋     | 20/43 [18:01<20:35, 53.73s/it] 49%|████▉     | 21/43 [18:55<19:43, 53.78s/it] 51%|█████     | 22/43 [19:49<18:49, 53.80s/it] 53%|█████▎    | 23/43 [20:43<17:56, 53.82s/it] 56%|█████▌    | 24/43 [21:37<17:03, 53.84s/it] 58%|█████▊    | 25/43 [22:30<16:09, 53.86s/it] 60%|██████    | 26/43 [23:24<15:15, 53.84s/it] 63%|██████▎   | 27/43 [24:18<14:20, 53.80s/it] 65%|██████▌   | 28/43 [25:12<13:26, 53.77s/it] 67%|██████▋   | 29/43 [26:05<12:33, 53.79s/it] 70%|██████▉   | 30/43 [26:59<11:39, 53.83s/it] 72%|███████▏  | 31/43 [27:53<10:46, 53.85s/it] 74%|███████▍  | 32/43 [28:47<09:51, 53.80s/it] 77%|███████▋  | 33/43 [29:41<08:57, 53.77s/it] 79%|███████▉  | 34/43 [30:34<08:03, 53.75s/it] 81%|████████▏ | 35/43 [31:28<07:09, 53.74s/it] 84%|████████▎ | 36/43 [32:22<06:16, 53.73s/it] 86%|████████▌ | 37/43 [33:16<05:22, 53.76s/it] 88%|████████▊ | 38/43 [34:09<04:28, 53.76s/it] 91%|█████████ | 39/43 [35:03<03:35, 53.78s/it] 93%|█████████▎| 40/43 [35:57<02:41, 53.79s/it] 95%|█████████▌| 41/43 [36:51<01:47, 53.76s/it] 98%|█████████▊| 42/43 [37:45<00:53, 53.78s/it]100%|██████████| 43/43 [38:22<00:00, 48.83s/it]100%|██████████| 43/43 [38:22<00:00, 53.54s/it]
Size (MB): 9116.626593
================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
LlamaForSequenceClassification                          --
├─LlamaModel: 1-1                                       --
│    └─embed_tokens.weight                              ├─525,340,672
│    └─layers.0.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.0.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.0.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.0.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.0.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.0.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.0.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.0.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.0.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.0.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.0.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.0.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.0.input_layernorm.weight                  ├─4,096
│    └─layers.0.post_attention_layernorm.weight         ├─4,096
│    └─layers.1.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.1.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.1.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.1.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.1.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.1.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.1.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.1.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.1.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.1.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.1.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.1.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.1.input_layernorm.weight                  ├─4,096
│    └─layers.1.post_attention_layernorm.weight         ├─4,096
│    └─layers.2.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.2.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.2.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.2.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.2.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.2.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.2.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.2.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.2.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.2.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.2.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.2.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.2.input_layernorm.weight                  ├─4,096
│    └─layers.2.post_attention_layernorm.weight         ├─4,096
│    └─layers.3.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.3.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.3.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.3.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.3.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.3.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.3.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.3.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.3.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.3.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.3.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.3.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.3.input_layernorm.weight                  ├─4,096
│    └─layers.3.post_attention_layernorm.weight         ├─4,096
│    └─layers.4.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.4.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.4.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.4.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.4.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.4.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.4.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.4.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.4.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.4.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.4.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.4.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.4.input_layernorm.weight                  ├─4,096
│    └─layers.4.post_attention_layernorm.weight         ├─4,096
│    └─layers.5.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.5.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.5.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.5.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.5.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.5.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.5.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.5.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.5.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.5.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.5.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.5.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.5.input_layernorm.weight                  ├─4,096
│    └─layers.5.post_attention_layernorm.weight         ├─4,096
│    └─layers.6.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.6.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.6.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.6.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.6.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.6.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.6.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.6.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.6.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.6.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.6.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.6.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.6.input_layernorm.weight                  ├─4,096
│    └─layers.6.post_attention_layernorm.weight         ├─4,096
│    └─layers.7.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.7.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.7.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.7.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.7.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.7.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.7.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.7.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.7.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.7.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.7.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.7.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.7.input_layernorm.weight                  ├─4,096
│    └─layers.7.post_attention_layernorm.weight         ├─4,096
│    └─layers.8.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.8.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.8.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.8.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.8.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.8.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.8.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.8.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.8.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.8.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.8.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.8.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.8.input_layernorm.weight                  ├─4,096
│    └─layers.8.post_attention_layernorm.weight         ├─4,096
│    └─layers.9.self_attn.q_proj.base_layer.weight      ├─16,777,216
│    └─layers.9.self_attn.q_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.q_proj.lora_B.default.weight  ├─131,072
│    └─layers.9.self_attn.k_proj.base_layer.weight      ├─4,194,304
│    └─layers.9.self_attn.k_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.k_proj.lora_B.default.weight  ├─32,768
│    └─layers.9.self_attn.v_proj.base_layer.weight      ├─4,194,304
│    └─layers.9.self_attn.v_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.v_proj.lora_B.default.weight  ├─32,768
│    └─layers.9.self_attn.o_proj.base_layer.weight      ├─16,777,216
│    └─layers.9.self_attn.o_proj.lora_A.default.weight  ├─131,072
│    └─layers.9.self_attn.o_proj.lora_B.default.weight  ├─131,072
│    └─layers.9.mlp.gate_proj.weight                    ├─58,720,256
│    └─layers.9.mlp.up_proj.weight                      ├─58,720,256
│    └─layers.9.mlp.down_proj.weight                    ├─58,720,256
│    └─layers.9.input_layernorm.weight                  ├─4,096
│    └─layers.9.post_attention_layernorm.weight         ├─4,096
│    └─layers.10.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.10.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.10.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.10.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.10.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.10.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.10.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.10.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.10.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.10.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.10.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.10.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.10.input_layernorm.weight                 ├─4,096
│    └─layers.10.post_attention_layernorm.weight        ├─4,096
│    └─layers.11.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.11.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.11.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.11.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.11.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.11.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.11.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.11.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.11.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.11.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.11.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.11.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.11.input_layernorm.weight                 ├─4,096
│    └─layers.11.post_attention_layernorm.weight        ├─4,096
│    └─layers.12.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.12.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.12.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.12.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.12.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.12.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.12.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.12.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.12.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.12.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.12.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.12.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.12.input_layernorm.weight                 ├─4,096
│    └─layers.12.post_attention_layernorm.weight        ├─4,096
│    └─layers.13.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.13.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.13.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.13.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.13.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.13.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.13.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.13.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.13.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.13.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.13.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.13.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.13.input_layernorm.weight                 ├─4,096
│    └─layers.13.post_attention_layernorm.weight        ├─4,096
│    └─layers.14.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.14.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.14.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.14.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.14.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.14.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.14.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.14.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.14.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.14.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.14.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.14.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.14.input_layernorm.weight                 ├─4,096
│    └─layers.14.post_attention_layernorm.weight        ├─4,096
│    └─layers.15.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.15.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.15.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.15.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.15.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.15.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.15.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.15.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.15.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.15.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.15.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.15.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.15.input_layernorm.weight                 ├─4,096
│    └─layers.15.post_attention_layernorm.weight        ├─4,096
│    └─layers.16.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.16.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.16.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.16.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.16.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.16.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.16.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.16.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.16.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.16.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.16.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.16.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.16.input_layernorm.weight                 ├─4,096
│    └─layers.16.post_attention_layernorm.weight        ├─4,096
│    └─layers.17.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.17.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.17.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.17.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.17.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.17.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.17.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.17.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.17.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.17.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.17.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.17.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.17.input_layernorm.weight                 ├─4,096
│    └─layers.17.post_attention_layernorm.weight        ├─4,096
│    └─layers.18.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.18.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.18.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.18.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.18.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.18.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.18.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.18.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.18.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.18.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.18.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.18.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.18.input_layernorm.weight                 ├─4,096
│    └─layers.18.post_attention_layernorm.weight        ├─4,096
│    └─layers.19.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.19.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.19.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.19.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.19.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.19.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.19.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.19.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.19.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.19.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.19.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.19.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.19.input_layernorm.weight                 ├─4,096
│    └─layers.19.post_attention_layernorm.weight        ├─4,096
│    └─layers.20.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.20.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.20.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.20.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.20.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.20.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.20.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.20.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.20.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.20.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.20.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.20.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.20.input_layernorm.weight                 ├─4,096
│    └─layers.20.post_attention_layernorm.weight        ├─4,096
│    └─layers.21.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.21.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.21.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.21.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.21.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.21.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.21.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.21.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.21.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.21.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.21.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.21.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.21.input_layernorm.weight                 ├─4,096
│    └─layers.21.post_attention_layernorm.weight        ├─4,096
│    └─layers.22.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.22.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.22.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.22.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.22.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.22.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.22.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.22.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.22.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.22.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.22.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.22.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.22.input_layernorm.weight                 ├─4,096
│    └─layers.22.post_attention_layernorm.weight        ├─4,096
│    └─layers.23.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.23.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.23.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.23.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.23.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.23.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.23.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.23.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.23.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.23.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.23.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.23.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.23.input_layernorm.weight                 ├─4,096
│    └─layers.23.post_attention_layernorm.weight        ├─4,096
│    └─layers.24.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.24.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.24.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.24.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.24.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.24.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.24.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.24.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.24.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.24.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.24.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.24.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.24.input_layernorm.weight                 ├─4,096
│    └─layers.24.post_attention_layernorm.weight        ├─4,096
│    └─layers.25.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.25.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.25.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.25.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.25.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.25.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.25.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.25.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.25.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.25.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.25.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.25.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.25.input_layernorm.weight                 ├─4,096
│    └─layers.25.post_attention_layernorm.weight        ├─4,096
│    └─layers.26.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.26.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.26.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.26.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.26.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.26.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.26.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.26.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.26.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.26.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.26.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.26.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.26.input_layernorm.weight                 ├─4,096
│    └─layers.26.post_attention_layernorm.weight        ├─4,096
│    └─layers.27.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.27.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.27.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.27.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.27.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.27.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.27.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.27.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.27.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.27.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.27.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.27.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.27.input_layernorm.weight                 ├─4,096
│    └─layers.27.post_attention_layernorm.weight        ├─4,096
│    └─layers.28.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.28.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.28.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.28.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.28.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.28.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.28.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.28.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.28.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.28.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.28.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.28.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.28.input_layernorm.weight                 ├─4,096
│    └─layers.28.post_attention_layernorm.weight        ├─4,096
│    └─layers.29.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.29.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.29.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.29.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.29.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.29.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.29.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.29.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.29.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.29.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.29.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.29.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.29.input_layernorm.weight                 ├─4,096
│    └─layers.29.post_attention_layernorm.weight        ├─4,096
│    └─layers.30.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.30.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.30.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.30.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.30.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.30.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.30.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.30.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.30.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.30.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.30.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.30.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.30.input_layernorm.weight                 ├─4,096
│    └─layers.30.post_attention_layernorm.weight        ├─4,096
│    └─layers.31.self_attn.q_proj.base_layer.weight     ├─16,777,216
│    └─layers.31.self_attn.q_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.q_proj.lora_B.default.weight ├─131,072
│    └─layers.31.self_attn.k_proj.base_layer.weight     ├─4,194,304
│    └─layers.31.self_attn.k_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.k_proj.lora_B.default.weight ├─32,768
│    └─layers.31.self_attn.v_proj.base_layer.weight     ├─4,194,304
│    └─layers.31.self_attn.v_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.v_proj.lora_B.default.weight ├─32,768
│    └─layers.31.self_attn.o_proj.base_layer.weight     ├─16,777,216
│    └─layers.31.self_attn.o_proj.lora_A.default.weight ├─131,072
│    └─layers.31.self_attn.o_proj.lora_B.default.weight ├─131,072
│    └─layers.31.mlp.gate_proj.weight                   ├─58,720,256
│    └─layers.31.mlp.up_proj.weight                     ├─58,720,256
│    └─layers.31.mlp.down_proj.weight                   ├─58,720,256
│    └─layers.31.input_layernorm.weight                 ├─4,096
│    └─layers.31.post_attention_layernorm.weight        ├─4,096
│    └─norm.weight                                      └─4,096
│    └─Embedding: 2-1                                   (525,340,672)
│    │    └─weight                                      └─525,340,672
│    └─ModuleList: 2-2                                  --
│    │    └─0.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─0.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─0.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─0.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─0.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─0.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─0.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─0.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─0.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─0.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─0.mlp.up_proj.weight                        ├─58,720,256
│    │    └─0.mlp.down_proj.weight                      ├─58,720,256
│    │    └─0.input_layernorm.weight                    ├─4,096
│    │    └─0.post_attention_layernorm.weight           ├─4,096
│    │    └─1.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─1.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─1.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─1.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─1.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─1.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─1.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─1.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─1.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─1.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─1.mlp.up_proj.weight                        ├─58,720,256
│    │    └─1.mlp.down_proj.weight                      ├─58,720,256
│    │    └─1.input_layernorm.weight                    ├─4,096
│    │    └─1.post_attention_layernorm.weight           ├─4,096
│    │    └─2.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─2.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─2.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─2.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─2.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─2.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─2.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─2.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─2.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─2.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─2.mlp.up_proj.weight                        ├─58,720,256
│    │    └─2.mlp.down_proj.weight                      ├─58,720,256
│    │    └─2.input_layernorm.weight                    ├─4,096
│    │    └─2.post_attention_layernorm.weight           ├─4,096
│    │    └─3.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─3.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─3.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─3.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─3.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─3.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─3.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─3.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─3.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─3.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─3.mlp.up_proj.weight                        ├─58,720,256
│    │    └─3.mlp.down_proj.weight                      ├─58,720,256
│    │    └─3.input_layernorm.weight                    ├─4,096
│    │    └─3.post_attention_layernorm.weight           ├─4,096
│    │    └─4.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─4.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─4.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─4.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─4.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─4.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─4.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─4.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─4.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─4.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─4.mlp.up_proj.weight                        ├─58,720,256
│    │    └─4.mlp.down_proj.weight                      ├─58,720,256
│    │    └─4.input_layernorm.weight                    ├─4,096
│    │    └─4.post_attention_layernorm.weight           ├─4,096
│    │    └─5.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─5.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─5.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─5.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─5.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─5.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─5.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─5.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─5.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─5.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─5.mlp.up_proj.weight                        ├─58,720,256
│    │    └─5.mlp.down_proj.weight                      ├─58,720,256
│    │    └─5.input_layernorm.weight                    ├─4,096
│    │    └─5.post_attention_layernorm.weight           ├─4,096
│    │    └─6.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─6.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─6.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─6.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─6.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─6.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─6.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─6.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─6.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─6.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─6.mlp.up_proj.weight                        ├─58,720,256
│    │    └─6.mlp.down_proj.weight                      ├─58,720,256
│    │    └─6.input_layernorm.weight                    ├─4,096
│    │    └─6.post_attention_layernorm.weight           ├─4,096
│    │    └─7.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─7.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─7.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─7.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─7.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─7.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─7.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─7.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─7.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─7.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─7.mlp.up_proj.weight                        ├─58,720,256
│    │    └─7.mlp.down_proj.weight                      ├─58,720,256
│    │    └─7.input_layernorm.weight                    ├─4,096
│    │    └─7.post_attention_layernorm.weight           ├─4,096
│    │    └─8.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─8.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─8.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─8.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─8.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─8.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─8.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─8.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─8.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─8.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─8.mlp.up_proj.weight                        ├─58,720,256
│    │    └─8.mlp.down_proj.weight                      ├─58,720,256
│    │    └─8.input_layernorm.weight                    ├─4,096
│    │    └─8.post_attention_layernorm.weight           ├─4,096
│    │    └─9.self_attn.q_proj.base_layer.weight        ├─16,777,216
│    │    └─9.self_attn.q_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.q_proj.lora_B.default.weight    ├─131,072
│    │    └─9.self_attn.k_proj.base_layer.weight        ├─4,194,304
│    │    └─9.self_attn.k_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.k_proj.lora_B.default.weight    ├─32,768
│    │    └─9.self_attn.v_proj.base_layer.weight        ├─4,194,304
│    │    └─9.self_attn.v_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.v_proj.lora_B.default.weight    ├─32,768
│    │    └─9.self_attn.o_proj.base_layer.weight        ├─16,777,216
│    │    └─9.self_attn.o_proj.lora_A.default.weight    ├─131,072
│    │    └─9.self_attn.o_proj.lora_B.default.weight    ├─131,072
│    │    └─9.mlp.gate_proj.weight                      ├─58,720,256
│    │    └─9.mlp.up_proj.weight                        ├─58,720,256
│    │    └─9.mlp.down_proj.weight                      ├─58,720,256
│    │    └─9.input_layernorm.weight                    ├─4,096
│    │    └─9.post_attention_layernorm.weight           ├─4,096
│    │    └─10.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─10.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─10.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─10.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─10.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─10.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─10.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─10.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─10.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─10.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─10.mlp.up_proj.weight                       ├─58,720,256
│    │    └─10.mlp.down_proj.weight                     ├─58,720,256
│    │    └─10.input_layernorm.weight                   ├─4,096
│    │    └─10.post_attention_layernorm.weight          ├─4,096
│    │    └─11.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─11.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─11.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─11.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─11.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─11.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─11.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─11.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─11.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─11.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─11.mlp.up_proj.weight                       ├─58,720,256
│    │    └─11.mlp.down_proj.weight                     ├─58,720,256
│    │    └─11.input_layernorm.weight                   ├─4,096
│    │    └─11.post_attention_layernorm.weight          ├─4,096
│    │    └─12.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─12.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─12.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─12.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─12.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─12.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─12.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─12.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─12.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─12.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─12.mlp.up_proj.weight                       ├─58,720,256
│    │    └─12.mlp.down_proj.weight                     ├─58,720,256
│    │    └─12.input_layernorm.weight                   ├─4,096
│    │    └─12.post_attention_layernorm.weight          ├─4,096
│    │    └─13.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─13.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─13.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─13.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─13.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─13.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─13.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─13.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─13.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─13.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─13.mlp.up_proj.weight                       ├─58,720,256
│    │    └─13.mlp.down_proj.weight                     ├─58,720,256
│    │    └─13.input_layernorm.weight                   ├─4,096
│    │    └─13.post_attention_layernorm.weight          ├─4,096
│    │    └─14.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─14.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─14.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─14.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─14.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─14.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─14.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─14.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─14.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─14.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─14.mlp.up_proj.weight                       ├─58,720,256
│    │    └─14.mlp.down_proj.weight                     ├─58,720,256
│    │    └─14.input_layernorm.weight                   ├─4,096
│    │    └─14.post_attention_layernorm.weight          ├─4,096
│    │    └─15.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─15.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─15.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─15.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─15.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─15.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─15.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─15.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─15.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─15.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─15.mlp.up_proj.weight                       ├─58,720,256
│    │    └─15.mlp.down_proj.weight                     ├─58,720,256
│    │    └─15.input_layernorm.weight                   ├─4,096
│    │    └─15.post_attention_layernorm.weight          ├─4,096
│    │    └─16.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─16.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─16.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─16.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─16.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─16.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─16.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─16.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─16.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─16.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─16.mlp.up_proj.weight                       ├─58,720,256
│    │    └─16.mlp.down_proj.weight                     ├─58,720,256
│    │    └─16.input_layernorm.weight                   ├─4,096
│    │    └─16.post_attention_layernorm.weight          ├─4,096
│    │    └─17.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─17.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─17.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─17.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─17.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─17.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─17.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─17.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─17.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─17.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─17.mlp.up_proj.weight                       ├─58,720,256
│    │    └─17.mlp.down_proj.weight                     ├─58,720,256
│    │    └─17.input_layernorm.weight                   ├─4,096
│    │    └─17.post_attention_layernorm.weight          ├─4,096
│    │    └─18.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─18.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─18.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─18.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─18.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─18.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─18.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─18.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─18.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─18.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─18.mlp.up_proj.weight                       ├─58,720,256
│    │    └─18.mlp.down_proj.weight                     ├─58,720,256
│    │    └─18.input_layernorm.weight                   ├─4,096
│    │    └─18.post_attention_layernorm.weight          ├─4,096
│    │    └─19.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─19.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─19.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─19.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─19.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─19.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─19.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─19.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─19.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─19.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─19.mlp.up_proj.weight                       ├─58,720,256
│    │    └─19.mlp.down_proj.weight                     ├─58,720,256
│    │    └─19.input_layernorm.weight                   ├─4,096
│    │    └─19.post_attention_layernorm.weight          ├─4,096
│    │    └─20.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─20.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─20.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─20.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─20.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─20.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─20.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─20.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─20.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─20.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─20.mlp.up_proj.weight                       ├─58,720,256
│    │    └─20.mlp.down_proj.weight                     ├─58,720,256
│    │    └─20.input_layernorm.weight                   ├─4,096
│    │    └─20.post_attention_layernorm.weight          ├─4,096
│    │    └─21.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─21.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─21.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─21.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─21.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─21.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─21.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─21.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─21.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─21.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─21.mlp.up_proj.weight                       ├─58,720,256
│    │    └─21.mlp.down_proj.weight                     ├─58,720,256
│    │    └─21.input_layernorm.weight                   ├─4,096
│    │    └─21.post_attention_layernorm.weight          ├─4,096
│    │    └─22.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─22.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─22.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─22.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─22.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─22.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─22.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─22.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─22.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─22.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─22.mlp.up_proj.weight                       ├─58,720,256
│    │    └─22.mlp.down_proj.weight                     ├─58,720,256
│    │    └─22.input_layernorm.weight                   ├─4,096
│    │    └─22.post_attention_layernorm.weight          ├─4,096
│    │    └─23.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─23.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─23.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─23.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─23.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─23.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─23.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─23.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─23.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─23.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─23.mlp.up_proj.weight                       ├─58,720,256
│    │    └─23.mlp.down_proj.weight                     ├─58,720,256
│    │    └─23.input_layernorm.weight                   ├─4,096
│    │    └─23.post_attention_layernorm.weight          ├─4,096
│    │    └─24.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─24.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─24.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─24.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─24.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─24.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─24.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─24.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─24.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─24.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─24.mlp.up_proj.weight                       ├─58,720,256
│    │    └─24.mlp.down_proj.weight                     ├─58,720,256
│    │    └─24.input_layernorm.weight                   ├─4,096
│    │    └─24.post_attention_layernorm.weight          ├─4,096
│    │    └─25.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─25.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─25.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─25.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─25.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─25.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─25.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─25.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─25.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─25.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─25.mlp.up_proj.weight                       ├─58,720,256
│    │    └─25.mlp.down_proj.weight                     ├─58,720,256
│    │    └─25.input_layernorm.weight                   ├─4,096
│    │    └─25.post_attention_layernorm.weight          ├─4,096
│    │    └─26.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─26.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─26.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─26.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─26.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─26.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─26.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─26.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─26.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─26.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─26.mlp.up_proj.weight                       ├─58,720,256
│    │    └─26.mlp.down_proj.weight                     ├─58,720,256
│    │    └─26.input_layernorm.weight                   ├─4,096
│    │    └─26.post_attention_layernorm.weight          ├─4,096
│    │    └─27.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─27.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─27.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─27.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─27.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─27.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─27.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─27.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─27.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─27.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─27.mlp.up_proj.weight                       ├─58,720,256
│    │    └─27.mlp.down_proj.weight                     ├─58,720,256
│    │    └─27.input_layernorm.weight                   ├─4,096
│    │    └─27.post_attention_layernorm.weight          ├─4,096
│    │    └─28.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─28.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─28.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─28.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─28.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─28.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─28.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─28.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─28.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─28.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─28.mlp.up_proj.weight                       ├─58,720,256
│    │    └─28.mlp.down_proj.weight                     ├─58,720,256
│    │    └─28.input_layernorm.weight                   ├─4,096
│    │    └─28.post_attention_layernorm.weight          ├─4,096
│    │    └─29.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─29.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─29.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─29.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─29.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─29.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─29.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─29.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─29.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─29.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─29.mlp.up_proj.weight                       ├─58,720,256
│    │    └─29.mlp.down_proj.weight                     ├─58,720,256
│    │    └─29.input_layernorm.weight                   ├─4,096
│    │    └─29.post_attention_layernorm.weight          ├─4,096
│    │    └─30.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─30.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─30.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─30.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─30.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─30.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─30.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─30.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─30.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─30.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─30.mlp.up_proj.weight                       ├─58,720,256
│    │    └─30.mlp.down_proj.weight                     ├─58,720,256
│    │    └─30.input_layernorm.weight                   ├─4,096
│    │    └─30.post_attention_layernorm.weight          ├─4,096
│    │    └─31.self_attn.q_proj.base_layer.weight       ├─16,777,216
│    │    └─31.self_attn.q_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.q_proj.lora_B.default.weight   ├─131,072
│    │    └─31.self_attn.k_proj.base_layer.weight       ├─4,194,304
│    │    └─31.self_attn.k_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.k_proj.lora_B.default.weight   ├─32,768
│    │    └─31.self_attn.v_proj.base_layer.weight       ├─4,194,304
│    │    └─31.self_attn.v_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.v_proj.lora_B.default.weight   ├─32,768
│    │    └─31.self_attn.o_proj.base_layer.weight       ├─16,777,216
│    │    └─31.self_attn.o_proj.lora_A.default.weight   ├─131,072
│    │    └─31.self_attn.o_proj.lora_B.default.weight   ├─131,072
│    │    └─31.mlp.gate_proj.weight                     ├─58,720,256
│    │    └─31.mlp.up_proj.weight                       ├─58,720,256
│    │    └─31.mlp.down_proj.weight                     ├─58,720,256
│    │    └─31.input_layernorm.weight                   ├─4,096
│    │    └─31.post_attention_layernorm.weight          └─4,096
│    │    └─LlamaDecoderLayer: 3-1                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-2                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-3                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-4                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-5                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-6                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-7                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-8                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-9                      218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-10                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-11                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-12                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-13                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-14                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-15                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-16                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-17                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-18                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-19                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-20                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-21                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-22                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-23                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-24                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-25                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-26                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-27                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-28                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-29                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-30                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-31                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    │    └─LlamaDecoderLayer: 3-32                     218,963,968
│    │    │    └─self_attn.q_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.q_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.q_proj.lora_B.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.k_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.k_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.v_proj.base_layer.weight     ├─4,194,304
│    │    │    └─self_attn.v_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.v_proj.lora_B.default.weight ├─32,768
│    │    │    └─self_attn.o_proj.base_layer.weight     ├─16,777,216
│    │    │    └─self_attn.o_proj.lora_A.default.weight ├─131,072
│    │    │    └─self_attn.o_proj.lora_B.default.weight ├─131,072
│    │    │    └─mlp.gate_proj.weight                   ├─58,720,256
│    │    │    └─mlp.up_proj.weight                     ├─58,720,256
│    │    │    └─mlp.down_proj.weight                   ├─58,720,256
│    │    │    └─input_layernorm.weight                 ├─4,096
│    │    │    └─post_attention_layernorm.weight        └─4,096
│    └─LlamaRMSNorm: 2-3                                (4,096)
│    │    └─weight                                      └─4,096
│    └─LlamaRotaryEmbedding: 2-4                        --
├─QLinear: 1-2                                          8,192
│    └─weight                                           └─8,192
================================================================================
Total params: 7,532,199,936
Trainable params: 7,006,593,024
Non-trainable params: 525,606,912
================================================================================
  0%|          | 0/43 [00:00<?, ?it/s]  2%|▏         | 1/43 [01:02<43:26, 62.06s/it]  5%|▍         | 2/43 [01:54<38:32, 56.40s/it]  7%|▋         | 3/43 [02:46<36:23, 54.59s/it]  9%|▉         | 4/43 [03:39<34:55, 53.73s/it] 12%|█▏        | 5/43 [04:31<33:44, 53.27s/it] 14%|█▍        | 6/43 [05:24<32:40, 52.98s/it] 16%|█▋        | 7/43 [06:16<31:40, 52.80s/it] 19%|█▊        | 8/43 [07:09<30:43, 52.68s/it] 21%|██        | 9/43 [08:01<29:48, 52.60s/it] 23%|██▎       | 10/43 [08:53<28:54, 52.55s/it] 26%|██▌       | 11/43 [09:46<28:00, 52.51s/it] 28%|██▊       | 12/43 [10:38<27:06, 52.48s/it] 30%|███       | 13/43 [11:31<26:14, 52.47s/it] 33%|███▎      | 14/43 [12:23<25:21, 52.46s/it] 35%|███▍      | 15/43 [13:16<24:28, 52.44s/it] 37%|███▋      | 16/43 [14:08<23:35, 52.44s/it] 40%|███▉      | 17/43 [15:00<22:43, 52.43s/it] 42%|████▏     | 18/43 [15:53<21:50, 52.43s/it] 44%|████▍     | 19/43 [16:45<20:58, 52.43s/it] 47%|████▋     | 20/43 [17:38<20:05, 52.42s/it] 49%|████▉     | 21/43 [18:30<19:13, 52.43s/it] 51%|█████     | 22/43 [19:23<18:21, 52.43s/it] 53%|█████▎    | 23/43 [20:15<17:28, 52.43s/it] 56%|█████▌    | 24/43 [21:07<16:36, 52.43s/it] 58%|█████▊    | 25/43 [22:00<15:43, 52.43s/it] 60%|██████    | 26/43 [22:52<14:51, 52.43s/it] 63%|██████▎   | 27/43 [23:45<13:58, 52.42s/it] 65%|██████▌   | 28/43 [24:37<13:06, 52.42s/it] 67%|██████▋   | 29/43 [25:29<12:13, 52.42s/it] 70%|██████▉   | 30/43 [26:22<11:21, 52.42s/it] 72%|███████▏  | 31/43 [27:14<10:29, 52.43s/it] 74%|███████▍  | 32/43 [28:07<09:36, 52.43s/it] 77%|███████▋  | 33/43 [28:59<08:44, 52.43s/it] 79%|███████▉  | 34/43 [29:52<07:51, 52.43s/it] 81%|████████▏ | 35/43 [30:44<06:59, 52.43s/it] 84%|████████▎ | 36/43 [31:37<06:07, 52.43s/it] 86%|████████▌ | 37/43 [32:29<05:14, 52.43s/it] 88%|████████▊ | 38/43 [33:21<04:22, 52.43s/it] 91%|█████████ | 39/43 [34:14<03:29, 52.43s/it] 93%|█████████▎| 40/43 [35:06<02:37, 52.42s/it] 95%|█████████▌| 41/43 [35:59<01:44, 52.42s/it] 98%|█████████▊| 42/43 [36:51<00:52, 52.43s/it]100%|██████████| 43/43 [37:27<00:00, 47.56s/it]100%|██████████| 43/43 [37:27<00:00, 52.27s/it]
Average inference time: 42.436750384264215
