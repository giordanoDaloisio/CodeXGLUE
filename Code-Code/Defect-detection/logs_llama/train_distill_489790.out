Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:  25%|██▌       | 1/4 [00:00<00:00,  9.04it/s]Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 30.62it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:20, 10.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  6.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.57s/it]
Some weights of the model checkpoint at meta-llama/Llama-3.1-8B were not used when initializing LlamaForSequenceClassification: ['lm_head.weight']
- This IS expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized because the shapes did not match:
- model.embed_tokens.weight: found shape torch.Size([128256, 4096]) in the checkpoint and torch.Size([128257, 4096]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Map:   0%|          | 0/2732 [00:00<?, ? examples/s]Map:   0%|          | 1/2732 [00:00<06:00,  7.57 examples/s]Map:   3%|▎         | 74/2732 [00:00<00:07, 379.65 examples/s]Map:   5%|▍         | 123/2732 [00:00<00:06, 423.66 examples/s]Map:   6%|▋         | 171/2732 [00:00<00:06, 369.57 examples/s]Map:   8%|▊         | 215/2732 [00:00<00:10, 247.21 examples/s]Map:  10%|▉         | 270/2732 [00:00<00:07, 308.89 examples/s]Map:  11%|█▏        | 314/2732 [00:00<00:07, 338.60 examples/s]Map:  14%|█▎        | 369/2732 [00:01<00:06, 346.01 examples/s]Map:  15%|█▌        | 423/2732 [00:01<00:06, 348.45 examples/s]Map:  17%|█▋        | 467/2732 [00:01<00:06, 368.84 examples/s]Map:  19%|█▉        | 514/2732 [00:01<00:05, 392.11 examples/s]Map:  20%|██        | 560/2732 [00:01<00:05, 405.86 examples/s]Map:  22%|██▏       | 604/2732 [00:01<00:05, 398.53 examples/s]Map:  24%|██▍       | 664/2732 [00:01<00:06, 342.02 examples/s]Map:  26%|██▌       | 707/2732 [00:02<00:06, 323.03 examples/s]Map:  27%|██▋       | 746/2732 [00:02<00:06, 330.01 examples/s]Map:  29%|██▊       | 784/2732 [00:02<00:05, 341.17 examples/s]Map:  30%|███       | 823/2732 [00:02<00:05, 352.21 examples/s]Map:  32%|███▏      | 869/2732 [00:02<00:05, 364.69 examples/s]Map:  33%|███▎      | 911/2732 [00:02<00:04, 369.08 examples/s]Map:  35%|███▍      | 955/2732 [00:02<00:05, 332.18 examples/s]Map:  37%|███▋      | 1000/2732 [00:03<00:10, 161.54 examples/s]Map:  39%|███▊      | 1053/2732 [00:03<00:08, 209.18 examples/s]Map:  41%|████      | 1107/2732 [00:03<00:06, 239.17 examples/s]Map:  42%|████▏     | 1156/2732 [00:03<00:05, 276.38 examples/s]Map:  44%|████▍     | 1208/2732 [00:03<00:04, 322.11 examples/s]Map:  46%|████▌     | 1262/2732 [00:04<00:04, 331.58 examples/s]Map:  48%|████▊     | 1321/2732 [00:04<00:03, 386.59 examples/s]Map:  52%|█████▏    | 1414/2732 [00:04<00:02, 511.54 examples/s]Map:  55%|█████▍    | 1489/2732 [00:04<00:02, 562.99 examples/s]Map:  57%|█████▋    | 1558/2732 [00:04<00:02, 525.10 examples/s]Map:  59%|█████▉    | 1619/2732 [00:04<00:02, 461.59 examples/s]Map:  61%|██████▏   | 1679/2732 [00:04<00:02, 436.64 examples/s]Map:  64%|██████▎   | 1738/2732 [00:04<00:02, 420.74 examples/s]Map:  66%|██████▌   | 1795/2732 [00:05<00:02, 404.48 examples/s]Map:  68%|██████▊   | 1853/2732 [00:05<00:02, 423.76 examples/s]Map:  71%|███████   | 1938/2732 [00:05<00:01, 521.38 examples/s]Map:  73%|███████▎  | 2000/2732 [00:05<00:01, 370.64 examples/s]Map:  76%|███████▌  | 2072/2732 [00:05<00:01, 431.25 examples/s]Map:  79%|███████▊  | 2148/2732 [00:05<00:01, 500.79 examples/s]Map:  82%|████████▏ | 2247/2732 [00:05<00:00, 595.04 examples/s]Map:  85%|████████▌ | 2331/2732 [00:06<00:00, 654.50 examples/s]Map:  88%|████████▊ | 2410/2732 [00:06<00:00, 688.96 examples/s]Map:  91%|█████████▏| 2499/2732 [00:06<00:00, 732.83 examples/s]Map:  95%|█████████▌| 2604/2732 [00:06<00:00, 714.02 examples/s]Map:  99%|█████████▉| 2714/2732 [00:06<00:00, 805.96 examples/s]Map: 100%|██████████| 2732/2732 [00:06<00:00, 408.05 examples/s]
{'idx': tensor([ 8, 10, 15, 22, 51, 62, 71, 75]), 'input_ids': tensor([[128000,   2020,    742,  ..., 128256, 128256, 128256],
        [128000,   2020,   7533,  ..., 128256, 128256, 128256],
        [128000,   1019,  34986,  ..., 128256, 128256, 128256],
        ...,
        [128000,   2020,    742,  ...,   1026,     72,     11],
        [128000,   1019,  77053,  ..., 128256, 128256, 128256],
        [128000,   2020,    742,  ...,   1390,    311,  17460]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([1, 1, 1, 1, 0, 0, 0, 0])}
Traceback (most recent call last):
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Code/Defect-detection/code/llama_distillation.py", line 99, in <module>
    main(args)
    ~~~~^^^^^^
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Code/Defect-detection/code/llama_distillation.py", line 78, in main
    student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/utils/generic.py", line 969, in wrapper
    output = func(self, *args, **kwargs)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py", line 789, in forward
    raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
ValueError: Cannot handle batch sizes > 1 if no padding token is defined.
srun: error: compute-2-3: task 0: Exited with exit code 1
