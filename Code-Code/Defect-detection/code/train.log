05/28/2025 09:57:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Traceback (most recent call last):
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Code/Defect-detection/code/run_lora.py", line 1089, in <module>
    main()
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Code/Defect-detection/code/run_lora.py", line 932, in main
    config = config_class.from_pretrained(
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.9/site-packages/transformers/configuration_utils.py", line 610, in from_pretrained
    return cls.from_dict(config_dict, **kwargs)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.9/site-packages/transformers/configuration_utils.py", line 772, in from_dict
    config = cls(**config_dict)
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.9/site-packages/transformers/models/llama/configuration_llama.py", line 161, in __init__
    self._rope_scaling_validation()
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.9/site-packages/transformers/models/llama/configuration_llama.py", line 182, in _rope_scaling_validation
    raise ValueError(
ValueError: `rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
