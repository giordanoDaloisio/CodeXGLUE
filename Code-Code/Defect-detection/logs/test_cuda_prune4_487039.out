03/24/2025 13:03:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/24/2025 13:03:50 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', model=None, mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=32, eval_batch_size=64, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, job_id='487039', quantize_dynamic=False, quantize_static=False, quantize=False, quantize4=False, quantizef8=False, prune_local=False, prune6=False, prune4=True, prune=False, attention_heads=None, hidden_dim=None, intermediate_size=None, n_layers=None, vocab_size=None, n_gpu=1, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=64, device=device(type='cuda'), start_epoch=0, start_step=0)
Traceback (most recent call last):
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Code/Defect-detection/code/run.py", line 1109, in <module>
    main()
  File "/NFSHOME/gdaloisio/code/CodeXGLUE/Code-Code/Defect-detection/code/run.py", line 1017, in main
    model.load_state_dict(torch.load(output_dir, map_location=device))
  File "/NFSHOME/gdaloisio/miniconda3/envs/codex/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2189, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for Model:
	Missing key(s) in state_dict: "encoder.roberta.encoder.layer.0.attention.self.query.weight", "encoder.roberta.encoder.layer.0.attention.self.key.weight", "encoder.roberta.encoder.layer.0.attention.self.value.weight", "encoder.roberta.encoder.layer.1.attention.self.query.weight", "encoder.roberta.encoder.layer.1.attention.self.key.weight", "encoder.roberta.encoder.layer.1.attention.self.value.weight", "encoder.roberta.encoder.layer.2.attention.self.query.weight", "encoder.roberta.encoder.layer.2.attention.self.key.weight", "encoder.roberta.encoder.layer.2.attention.self.value.weight", "encoder.roberta.encoder.layer.3.attention.self.query.weight", "encoder.roberta.encoder.layer.3.attention.self.key.weight", "encoder.roberta.encoder.layer.3.attention.self.value.weight", "encoder.roberta.encoder.layer.4.attention.self.query.weight", "encoder.roberta.encoder.layer.4.attention.self.key.weight", "encoder.roberta.encoder.layer.4.attention.self.value.weight", "encoder.roberta.encoder.layer.5.attention.self.query.weight", "encoder.roberta.encoder.layer.5.attention.self.key.weight", "encoder.roberta.encoder.layer.5.attention.self.value.weight", "encoder.roberta.encoder.layer.6.attention.self.query.weight", "encoder.roberta.encoder.layer.6.attention.self.key.weight", "encoder.roberta.encoder.layer.6.attention.self.value.weight", "encoder.roberta.encoder.layer.7.attention.self.query.weight", "encoder.roberta.encoder.layer.7.attention.self.key.weight", "encoder.roberta.encoder.layer.7.attention.self.value.weight", "encoder.roberta.encoder.layer.8.attention.self.query.weight", "encoder.roberta.encoder.layer.8.attention.self.key.weight", "encoder.roberta.encoder.layer.8.attention.self.value.weight", "encoder.roberta.encoder.layer.9.attention.self.query.weight", "encoder.roberta.encoder.layer.9.attention.self.key.weight", "encoder.roberta.encoder.layer.9.attention.self.value.weight", "encoder.roberta.encoder.layer.10.attention.self.query.weight", "encoder.roberta.encoder.layer.10.attention.self.key.weight", "encoder.roberta.encoder.layer.10.attention.self.value.weight", "encoder.roberta.encoder.layer.11.attention.self.query.weight", "encoder.roberta.encoder.layer.11.attention.self.key.weight", "encoder.roberta.encoder.layer.11.attention.self.value.weight". 
	Unexpected key(s) in state_dict: "encoder.roberta.encoder.layer.0.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.0.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.0.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.0.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.0.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.0.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.1.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.1.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.1.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.1.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.1.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.1.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.2.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.2.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.2.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.2.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.2.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.2.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.3.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.3.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.3.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.3.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.3.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.3.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.4.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.4.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.4.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.4.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.4.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.4.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.5.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.5.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.5.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.5.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.5.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.5.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.6.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.6.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.6.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.6.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.6.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.6.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.7.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.7.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.7.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.7.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.7.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.7.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.8.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.8.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.8.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.8.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.8.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.8.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.9.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.9.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.9.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.9.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.9.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.9.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.10.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.10.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.10.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.10.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.10.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.10.attention.self.value.weight_mask", "encoder.roberta.encoder.layer.11.attention.self.query.weight_orig", "encoder.roberta.encoder.layer.11.attention.self.query.weight_mask", "encoder.roberta.encoder.layer.11.attention.self.key.weight_orig", "encoder.roberta.encoder.layer.11.attention.self.key.weight_mask", "encoder.roberta.encoder.layer.11.attention.self.value.weight_orig", "encoder.roberta.encoder.layer.11.attention.self.value.weight_mask". 
srun: error: compute-0-3: task 0: Exited with exit code 1
