06/14/2024 18:18:28 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /NFSHOME/gdaloisio/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
06/14/2024 18:18:28 - INFO - transformers.configuration_utils -   Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

06/14/2024 18:18:29 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /NFSHOME/gdaloisio/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
06/14/2024 18:18:29 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /NFSHOME/gdaloisio/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
06/14/2024 18:18:29 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/microsoft/codebert-base-mlm/pytorch_model.bin from cache at /NFSHOME/gdaloisio/.cache/torch/transformers/784f9c7b80cd9199c04c4b21b316954cccca4a5d01682e038b1ec3ef2bbb3c79.531138efbdd8d92615a1124a934e1e14c015d558302b8e917f88bffe84795427
06/14/2024 18:18:36 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing RobertaForMaskedLM.

06/14/2024 18:18:36 - INFO - transformers.modeling_utils -   All the weights of RobertaForMaskedLM were initialized from the model checkpoint at microsoft/codebert-base-mlm.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
06/14/2024 18:18:36 - INFO - __main__ -   Running on cpu
06/14/2024 18:18:36 - INFO - __main__ -   cloze test mode: all
Traceback (most recent call last):
  File "run_cloze.py", line 89, in <module>
    main()
  File "run_cloze.py", line 85, in main
    cloze_results.extend(cloze_test(args, model, tokenizer, device))
  File "run_cloze.py", line 59, in cloze_test
    with open(os.path.join(args.output_dir, args.lang, 'predictions.txt'), 'w', encoding='utf-8') as fp:
FileNotFoundError: [Errno 2] No such file or directory: 'evaluator/predictions/ruby/predictions.txt'
srun: error: compute-0-7: task 0: Exited with exit code 1
