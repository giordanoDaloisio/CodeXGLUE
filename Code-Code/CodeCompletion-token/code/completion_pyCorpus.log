Model has a total of 124422912 trainable parameters
Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='py150', output_dir='../save/py150', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_pyCorpus.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../dataset/py150/token_completion/train.txt
Data size: 95000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Model has a total of 124422912 trainable parameters
Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='py150', output_dir='../save/py150', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_pyCorpus.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../dataset/py150/token_completion/train.txt
Data size: 32585
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 41965898
Rank 0 Training 41965898 token, 40982 samples
Saving features into cached file ../save/py150/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 40982
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 25610
  steps: 100  ppl: 4.169  lr: 7.96876220226474e-05
  steps: 200  ppl: 3.1488  lr: 7.937524404529481e-05
  steps: 300  ppl: 2.976  lr: 7.906286606794221e-05
  steps: 400  ppl: 2.9436  lr: 7.875048809058962e-05
  steps: 500  ppl: 2.9656  lr: 7.843811011323702e-05
  steps: 600  ppl: 2.8857  lr: 7.812573213588442e-05
  steps: 700  ppl: 2.8564  lr: 7.781335415853183e-05
  steps: 800  ppl: 2.8277  lr: 7.750097618117923e-05
  steps: 900  ppl: 2.8455  lr: 7.718859820382664e-05
  steps: 1000  ppl: 2.7494  lr: 7.687622022647403e-05
Data size: 5000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
tokens: 7577235
  perplexity = 2.7431
Saving model checkpoint to ../save/py150/checkpoint-1000-2.7431
Saving optimizer and scheduler states to ../save/py150/checkpoint-last
  steps: 1100  ppl: 2.8116  lr: 7.656384224912144e-05
  steps: 1200  ppl: 2.6884  lr: 7.625146427176884e-05
  steps: 1300  ppl: 2.7429  lr: 7.593908629441625e-05
  steps: 1400  ppl: 2.7616  lr: 7.562670831706365e-05
  steps: 1500  ppl: 2.7049  lr: 7.531433033971106e-05
  steps: 1600  ppl: 2.7332  lr: 7.500195236235846e-05
  steps: 1700  ppl: 2.661  lr: 7.468957438500587e-05
  steps: 1800  ppl: 2.6878  lr: 7.437719640765327e-05
  steps: 1900  ppl: 2.6352  lr: 7.406481843030068e-05
  steps: 2000  ppl: 2.6187  lr: 7.375244045294807e-05
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='py150', output_dir='../save/py150', model_type='gpt2', pretrain_dir='../save/py150/checkpoint-last', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_pyCorpus.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=1000, config_name='../save/py150/checkpoint-last/config.json')
Loading features from cached file ../save/py150/train_blocksize_1024_wordsize_1_rank_0
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
Student Model has a total of 124422912 trainable parameters
Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='py150', output_dir='../save/py150', model_type='gpt2', pretrain_dir='../save/py150/checkpoint-last', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_pyCorpus.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=1000, config_name='../save/py150/checkpoint-last/config.json')
Loading features from cached file ../save/py150/train_blocksize_1024_wordsize_1_rank_0
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
Student Model has a total of 124422912 trainable parameters
Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='py150', output_dir='../save/py150', model_type='gpt2', pretrain_dir='../save/py150/checkpoint-last', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_pyCorpus.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=1000, config_name='../save/py150/checkpoint-last/config.json')
Loading features from cached file ../save/py150/train_blocksize_1024_wordsize_1_rank_0
Loading optimizer from ../save/py150/checkpoint-last/optimizer.pt
***** Running training *****
  Num examples = 40982
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 25610
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
Student Model has a total of 124422912 trainable parameters
Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='py150', output_dir='../save/py150', model_type='gpt2', pretrain_dir='../save/py150/checkpoint-last', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_pyCorpus.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=1000, config_name='../save/py150/checkpoint-last/config.json')
Loading features from cached file ../save/py150/train_blocksize_1024_wordsize_1_rank_0
Loading optimizer from ../save/py150/checkpoint-last/optimizer.pt
***** Running training *****
  Num examples = 40982
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 25610
reload model from ../save/py150/checkpoint-last, resume from 1000 steps
Model has a total of 124422912 trainable parameters
Student Model has a total of 124422912 trainable parameters
Training/evaluation parameters Namespace(data_dir='../dataset/py150/token_completion', langs='py150', output_dir='../save/py150', model_type='gpt2', pretrain_dir='../save/py150/checkpoint-last', config_dir=None, tokenizer_dir=None, lit_file='../dataset/py150/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_pyCorpus.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=1000, config_name='../save/py150/checkpoint-last/config.json')
Loading features from cached file ../save/py150/train_blocksize_1024_wordsize_1_rank_0
Loading optimizer from ../save/py150/checkpoint-last/optimizer.pt
***** Running training *****
  Num examples = 40982
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 25610
  steps: 1100  ppl: 146.1168  lr: 7.96876220226474e-05
  steps: 1200  ppl: 42.4207  lr: 7.937524404529481e-05
  steps: 1300  ppl: 32.1297  lr: 7.906286606794221e-05
  steps: 1400  ppl: 31.625  lr: 7.875048809058962e-05
  steps: 1500  ppl: 29.4636  lr: 7.843811011323702e-05
  steps: 1600  ppl: 28.4113  lr: 7.812573213588442e-05
  steps: 1700  ppl: 26.9839  lr: 7.781335415853183e-05
  steps: 1800  ppl: 24.8837  lr: 7.750097618117923e-05
  steps: 1900  ppl: 25.2122  lr: 7.718859820382664e-05
  steps: 2000  ppl: 21.8634  lr: 7.687622022647403e-05
  perplexity = 23.1428
Saving model checkpoint to ../save/py150/checkpoint-2000-23.1428
Saving optimizer and scheduler states to ../save/py150/checkpoint-last
